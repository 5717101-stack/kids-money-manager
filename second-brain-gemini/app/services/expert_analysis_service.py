"""
Expert Analysis Service - Council of Experts for Meeting Analysis

This service applies expert personas to analyze transcribed conversations:
- Michal Dalyot / Adler Institute (Parenting/Boundaries)
- Esther Perel (Relationships/Communication)
- McKinsey & Co. / Israeli Tech (Business/Strategy)
- Simon Sinek (Leadership/The Why)

Flow:
1. Context Detection - Gemini categorizes the conversation
2. Expert Assignment - 1-2 most relevant personas assigned
3. Deep Analysis - Who said what, sentiment, insights
4. Mandatory Kaizen - Preserve/Improve feedback

Each persona provides:
- Sentiment analysis
- Executive summary (deep attribution)
- Expert insights from assigned persona(s)
- Action items (assigned to specific speakers)
- Mandatory Kaizen Feedback (×œ×©×™××•×¨ / ×œ×©×™×¤×•×¨)
"""

# ============================================================================
# DIRECT AUDIO ANALYSIS PROMPT
# This is the proven prompt from process_meetings.py that works reliably
# for Drive Inbox uploads. We use it for WhatsApp audio as well.
# ============================================================================
DIRECT_AUDIO_SYSTEM_INSTRUCTION = """You are an expert AI assistant with access to multiple expert personas. Listen to the attached audio meeting and generate a Hebrew summary using a sophisticated Multi-Agent System.

Step 1: CONTEXT & SPEAKER IDENTIFICATION

First, identify the speakers. Specifically look for:
- Itzik (Me)
- Eran (Husband/Partner)

Then, classify the conversation context:

If the conversation is between Itzik and Eran about their relationship, feelings, or shared life -> Flag as COUPLE_DYNAMICS (unless they explicitly talk about kids only).

If the conversation is about raising children/home logistics -> Flag as PARENTING.

If the conversation is about team culture/leadership/mentoring -> Flag as LEADERSHIP.

If the conversation is about business strategy, product decisions, or roadmap -> Flag as STRATEGY.

Step 2: SELECT THE EXPERT PERSONA

Based on the flag, adopt a specific mental framework for the analysis:

RELATIONSHIP (Esther Perel Mode):
- Trigger: Discussions between Itzik & Eran about their relationship, feelings, or shared life.
- Focus: Emotional intelligence, balance between security and freedom, listening to the "unsaid", reconciling desire with domestic life.
- Tone: Empathetic, insightful, deep.

STRATEGY (McKinsey + Tech Innovation Mode):
- Trigger: Business decisions, product roadmap, tech strategy.
- Focus: "MECE" (Mutually Exclusive, Collectively Exhaustive) structure, data-driven insights, scalability, combined with Agile/Lean Startup thinking (MVP, iteration, speed).
- Tone: Sharp, professional, action-oriented, cutting through the noise.

LEADERSHIP (Simon Sinek Mode):
- Trigger: Team management, hiring, mentoring, culture.
- Focus: "Start with Why", The Infinite Game, creating a Circle of Safety, leaders eat last.
- Tone: Inspiring, human-centric, visionary.

PARENTING (Adler Institute Mode):
- Trigger: Kids, education, home rules.
- Focus: Encouragement, natural consequences, cooperation, avoiding power struggles.
- Tone: Supportive, practical, educational.

Step 3: GENERATE THE HEBREW OUTPUT

Structure the response strictly as follows (add relevant emojis):

ğŸ§  ×”×›×•×‘×¢ ×©× ×‘×—×¨: [Name of the Expert/Mode used - e.g., "××¡×ª×¨ ×¤×¨×œ (×™×—×¡×™×)", "××§×™× ×–×™ + Tech Innovation (××¡×˜×¨×˜×’×™×”)", "×¡×™×™××•×Ÿ ×¡×™× ×§ (×× ×”×™×’×•×ª)", "××›×•×Ÿ ××“×œ×¨ (×”×•×¨×•×ª)"]

ğŸ“Œ × ×•×©× ×”×©×™×—×”: [Concise Subject - 3-5 words]

ğŸ•µï¸ ×”×¡××‘-×˜×§×¡×˜ (× ×™×ª×•×— ×¢×•××§): [2-3 sentences analyzing NOT just what was said, but the underlying dynamics/principles based on the chosen expert persona. Go deep into what's really happening beneath the surface.]

ğŸ’¡ ×ª×•×‘× ×” ××¨×›×–×™×ª (The Insight): [The single most valuable takeaway using the expert's specific terminology and framework. This should be the "aha moment" that the expert would highlight.]

âš–ï¸ ××“×“ ×”×‘×”×™×¨×•×ª / ×˜×™×‘ ×”×™×—×¡×™×: 
[If Strategy: Rate clarity of decision 1-10 with brief explanation]
[If Relationship: Rate quality of communication 1-10 with brief explanation]
[If Leadership: Rate effectiveness of leadership approach 1-10 with brief explanation]
[If Parenting: Rate quality of parenting approach 1-10 with brief explanation]

âœ… ××§×©×Ÿ ××™×™×˜××¡ (×ª×›×œ×¡):
[Task 1 - specific and actionable]
[Task 2 - specific and actionable]
(Or "××™×Ÿ ××©×™××•×ª ×œ×”××©×š" if no action items)

ğŸ“ˆ ×§××™×–×Ÿ - ×¤×™×“×‘×§ ×œ×¦××™×—×”:
âœ“ ×œ×©×™××•×¨: [One specific positive behavior/decision to preserve]
â†’ ×œ×©×™×¤×•×¨: [One MANDATORY area for growth - always find something]

â“ ×©××œ×” ×œ××—×©×‘×” (Reflection): [One provocative/hard question that the expert would ask to help grow. This should challenge assumptions and encourage deeper thinking.]

CRITICAL: The entire output must be in Hebrew. Use the expert's specific terminology and framework throughout. Be insightful, not just descriptive. Keep the total response under 1500 characters for WhatsApp compatibility.
"""

import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta

import google.generativeai as genai

from app.core.config import settings
from app.services.knowledge_base_service import get_system_instruction_block as get_kb_context

logger = logging.getLogger(__name__)

# Israel timezone offset (UTC+2 winter, UTC+3 summer)
ISRAEL_TZ_OFFSET = timedelta(hours=2)


def get_israel_time() -> datetime:
    """Get current time in Israel timezone."""
    utc_now = datetime.now(timezone.utc)
    return utc_now + ISRAEL_TZ_OFFSET


# Council of Experts - Persona Definitions
EXPERT_PERSONAS = {
    "parenting": {
        "name": "××™×›×œ ×“×œ×™×•×ª / ××›×•×Ÿ ××“×œ×¨ (×”×•×¨×•×ª ×•×’×‘×•×œ×•×ª)",
        "short_name": "××›×•×Ÿ ××“×œ×¨",
        "category": "Parenting",
        "trigger_keywords": [
            "×™×œ×“×™×", "×‘× ×™×", "×‘× ×•×ª", "×‘×™×ª", "×—×™× ×•×š", "××˜×œ×•×ª", "×©×™×¢×•×¨×™ ×‘×™×ª",
            "kids", "children", "×”×•×¨×™×", "××©×¤×—×”", "×’×‘×•×œ×•×ª", "×›×œ×œ×™×",
            "×™×œ×“", "×™×œ×“×”", "×‘×Ÿ", "×‘×ª", "×”×ª× ×”×’×•×ª", "××©××¢×ª", "×”×•×¨×•×ª",
            "×’×Ÿ", "×‘×™×ª ×¡×¤×¨", "××•×¨×”", "×˜×™×¤×•×œ", "×—×•××”", "×œ×™×œ×”", "×©×™× ×”",
            "××•×›×œ", "×¡×œ×•×œ×¨", "××¡×›×™×", "×™×—×“", "××—×™×", "××—×™×•×ª"
        ],
        "focus": """
×’×™×©×ª ××“×œ×¨ ×œ×”×•×¨×•×ª ×“××•×§×¨×˜×™×ª:
- ×¢×™×“×•×“ ×‘××§×•× ×©×‘×— (Encouragement vs Praise)
- ×ª×•×¦××•×ª ×˜×‘×¢×™×•×ª ×•×”×’×™×•× ×™×•×ª (Natural & Logical Consequences)
- ×©×™×ª×•×£ ×¤×¢×•×œ×” ×•××—×¨×™×•×ª ××©×•×ª×¤×ª
- ×”×™×× ×¢×•×ª ××××‘×§×™ ×›×•×—
- ×›×‘×•×“ ×”×“×“×™ ×•×’×‘×•×œ×•×ª ×‘×¨×•×¨×™×
- ××ª×Ÿ ×‘×—×™×¨×•×ª ×‘××¡×’×¨×ª ×’×‘×•×œ×•×ª
- ×–×™×”×•×™ "××˜×¨×•×ª ××•×˜×¢×•×ª" ×‘×”×ª× ×”×’×•×ª
""",
        "tone": "×ª×•××š, ×¤×¨×§×˜×™, ×—×™× ×•×›×™, ×œ×œ× ×©×™×¤×•×˜×™×•×ª",
        "key_questions": [
            "×”×× ×”×©×™×—×” ××©×ª××©×ª ×‘×©×¤×” ××¢×•×“×“×ª ××• ×‘×‘×™×§×•×¨×ª?",
            "×”×× ×™×© ×××‘×§×™ ×›×•×— × ×¡×ª×¨×™×?",
            "×”×× ×”×’×‘×•×œ×•×ª ×©×”×•×’×“×¨×• ×‘×¨×•×¨×™× ×•×˜×‘×¢×™×™×?"
        ]
    },
    "relationship": {
        "name": "××¡×ª×¨ ×¤×¨×œ (×™×—×¡×™× ×•×ª×§×©×•×¨×ª)",
        "short_name": "××¡×ª×¨ ×¤×¨×œ",
        "category": "Relationship",
        "trigger_keywords": [
            "×‘×¢×œ", "××™×©×”", "×–×•×’×™×•×ª", "×¨×’×©×•×ª", "××”×‘×”", "relationship", "partner",
            "×¢×¨×Ÿ", "×–×•×’", "×›×¢×¡", "×ª×¡×›×•×œ", "××™× ×˜×™××™×•×ª", "×§×©×¨", "×—×™×‘×•×¨",
            "×¨×™×‘", "×•×™×›×•×—", "×œ× ××‘×™×Ÿ", "×œ× ××‘×™× ×”", "××¨×—×§", "×§×¨×‘×”",
            "×—×•×¤×©×”", "×–×•×’×™", "×‘×™×—×“", "× ×™×©×•××™×Ÿ", "×©×•×ª×¤×•×ª", "×ª××™×›×”"
        ],
        "focus": """
×’×™×©×ª ××¡×ª×¨ ×¤×¨×œ ×œ×–×•×’×™×•×ª:
- ××™× ×˜×œ×™×’× ×¦×™×” ×¨×’×©×™×ª ×¢××•×§×”
- ××™×–×•×Ÿ ×‘×™×Ÿ ×‘×™×˜×—×•×Ÿ (Security) ×œ×—×•×¤×© (Freedom)
- ×”×§×©×‘×” ×œ"×œ× × ×××¨" - ××” ×‘×™×Ÿ ×”×©×•×¨×•×ª
- ×”×‘× ×ª ×“×™× ××™×§×•×ª ×›×•×— × ×¡×ª×¨×•×ª
- ×¤×™×©×•×¨ ×‘×™×Ÿ ×ª×©×•×§×” ×œ×—×™×™ ×™×•×-×™×•×
- ×–×™×”×•×™ ×“×¤×•×¡×™× ×—×•×–×¨×™× ×‘×ª×§×©×•×¨×ª
- ×”×¢×¨×›×ª "×¡×’× ×•× ×•×ª ×”×ª×§×©×¨×•×ª" (Attachment Styles)
""",
        "tone": "×××¤×ª×™, ×¢××•×§, ×ª×•×‘× ×ª×™, ×œ×œ× ×©×™×¤×•×˜",
        "key_questions": [
            "××” ×œ× × ×××¨ ×‘×©×™×—×”?",
            "××™×–×” ×¨×’×© × ×¡×ª×¨ ××ª×—×ª ×œ×¤× ×™ ×”×©×˜×—?",
            "×”×× ×™×© ×“×¤×•×¡ ×—×•×–×¨ ×©×¨××•×™ ×œ×©×™× ×œ×‘ ××œ×™×•?"
        ]
    },
    "strategy": {
        "name": "××§×™× ×–×™ ×•×˜×§ ×™×©×¨××œ×™ (××¡×˜×¨×˜×’×™×” ×•×¢×¡×§×™×)",
        "short_name": "××§×™× ×–×™ + Tech",
        "category": "Business",
        "trigger_keywords": [
            "×¢×¡×§", "××•×¦×¨", "×œ×§×•×—×•×ª", "××¡×˜×¨×˜×’×™×”", "roadmap", "MVP", "startup",
            "product", "×›×¡×£", "×ª×§×¦×™×‘", "×”×›× ×¡×•×ª", "××›×™×¨×•×ª", "×©×•×§", "××ª×—×¨×™×",
            "×¤×™×¦'×¨", "×¤×™×ª×•×—", "tech", "×§×•×“", "××©×§×™×¢×™×", "×’×™×•×¡", "×—×‘×¨×”",
            "×¢×¡×§×™×", "×¤×¨×•×™×§×˜", "×“×“×œ×™×™×Ÿ", "×™×¢×“×™×", "KPI", "OKR", "×‘×™×¦×•×¢×™×",
            "×ª×›× ×•×Ÿ", "××¤×œ×™×§×¦×™×”", "××ª×¨", "×©×™×¨×•×ª", "B2B", "B2C", "SaaS"
        ],
        "focus": """
×’×™×©×ª ××§×™× ×–×™ + High-Tech ×™×©×¨××œ×™:
- ××‘× ×” MECE (Mutually Exclusive, Collectively Exhaustive)
- × ×™×ª×•×— ××‘×•×¡×¡ × ×ª×•× ×™× (Data-Driven)
- ×¡×§×™×™×œ×‘×™×œ×™×•×ª ×•×™×¢×™×œ×•×ª
- ×—×©×™×‘×ª Agile/Lean Startup
- MVP ×•××™×˜×¨×¦×™×•×ª ××”×™×¨×•×ª
- ×”×’×“×¨×ª KPIs ×‘×¨×•×¨×™×
- ×ª×¢×“×•×£ ××›×–×¨×™ (Ruthless Prioritization)
- "First Principles Thinking"
""",
        "tone": "×—×“, ××§×¦×•×¢×™, ×××•×§×“ ×¤×¢×•×œ×”, ×—×•×ª×š ×œ×¢× ×™×™×Ÿ",
        "key_questions": [
            "××” ×”-ROI ×”×¦×¤×•×™?",
            "××” ×”×¦×¢×“ ×”×‘× ×”×§×˜×Ÿ ×‘×™×•×ª×¨ ×©××‘×™× ×¢×¨×š?",
            "××” ×”-bottleneck ×”×××™×ª×™?"
        ]
    },
    "leadership": {
        "name": "×¡×™×™××•×Ÿ ×¡×™× ×§ (×× ×”×™×’×•×ª ×•×”×©×¨××”)",
        "short_name": "×¡×™×™××•×Ÿ ×¡×™× ×§",
        "category": "Leadership",
        "trigger_keywords": [
            "×¦×•×•×ª", "×¢×•×‘×“×™×", "×× ×”×œ", "×—×‘×¨×”", "×ª×¨×‘×•×ª", "hiring", "mentoring",
            "culture", "×× ×”×™×’×•×ª", "×”×©×¨××”", "×¢×¨×›×™×", "×—×–×•×Ÿ", "××©××¢×•×ª",
            "××•×˜×™×‘×¦×™×”", "×’×™×•×¡", "×¤×™×˜×•×¨×™×", "×‘×™×¦×•×¢×™×", "××©×•×‘", "feedback",
            "1on1", "××—×“ ×¢×œ ××—×“", "×¦××™×—×”", "×§×¨×™×™×¨×”", "×¤×•×˜× ×¦×™××œ"
        ],
        "focus": """
×’×™×©×ª ×¡×™×™××•×Ÿ ×¡×™× ×§ ×œ×× ×”×™×’×•×ª:
- Start with Why - ×ª×ª×—×™×œ ××”"×œ××”"
- The Infinite Game - ××©×—×§ ××™× ×¡×•×¤×™, ×œ× × ×™×¦×—×•×Ÿ ×—×“ ×¤×¢××™
- Circle of Safety - ×™×¦×™×¨×ª ××¢×’×œ ×‘×™×˜×—×•×Ÿ ×œ×¦×•×•×ª
- Leaders Eat Last - ×× ×”×™×’ ×“×•××’ ×§×•×“× ×œ××—×¨×™×
- ×× ×”×™×’×•×ª ××©×¨×ª×ª (Servant Leadership)
- ×‘× ×™×™×ª ×××•×Ÿ ×œ×˜×•×•×— ××¨×•×š
- "Optimism fueled by reality"
""",
        "tone": "××¢×•×¨×¨ ×”×©×¨××”, ×××•×§×“ ×‘×× ×©×™×, ×—×–×•× ×™, ×× ×•×©×™",
        "key_questions": [
            "××” ×”'×œ××”' ×××—×•×¨×™ ×”×”×—×œ×˜×”?",
            "×”×× ×–×” ×‘×•× ×” ×××•×Ÿ ×œ×˜×•×•×— ××¨×•×š?",
            "×”×× ×”×× ×”×™×’×•×ª ×›××Ÿ ××©×¨×ª×ª ××• ×©×•×œ×˜×ª?"
        ]
    },
    "general": {
        "name": "×¢×•×–×¨ ××™×©×™ ×—×›×",
        "short_name": "×¢×•×–×¨ ××™×©×™",
        "category": "General",
        "trigger_keywords": [],
        "focus": "×¡×™×›×•× ×‘×¨×•×¨ ×•×ª××¦×™×ª×™ ×¢× ×ª×•×‘× ×•×ª ×¤×¨×§×˜×™×•×ª ×•××§×©×Ÿ ××™×™×˜××¡",
        "tone": "×™×©×™×¨, ×©×™××•×©×™, ×¤×¨×§×˜×™",
        "key_questions": ["××” ×”× ×§×•×“×•×ª ×”×¢×™×§×¨×™×•×ª?", "××” ×”×¦×¢×“ ×”×‘×?"]
    }
}


class ExpertAnalysisService:
    """
    Council of Experts Analysis System for meeting transcripts.
    
    Flow:
    1. Context Detection - Ask Gemini to categorize the transcript
    2. Expert Assignment - Select 1-2 relevant personas
    3. Deep Analysis - Full analysis with attribution
    4. Mandatory Kaizen - Preserve/Improve feedback
    """
    
    def __init__(self):
        self.api_key = settings.google_api_key
        self.model = None
        self.model_name = None
        
        if self.api_key:
            from app.services.model_discovery import configure_genai, get_best_model, MODEL_MAPPING
            configure_genai(self.api_key)
            
            model_name = get_best_model(MODEL_MAPPING["pro"], category="general")
            if model_name:
                self.model = genai.GenerativeModel(model_name)
                self.model_name = model_name
                logger.info(f"âœ… ×©×™×¨×•×ª × ×™×ª×•×— ×”××•××—×™× ××•×ª×—×œ ×¢× {model_name}")
                print(f"âœ… [Expert Analysis] Model initialized: {model_name}")
            else:
                logger.error("âŒ No model found for Expert Analysis service")
                print("âŒ [Expert Analysis] No model found via discovery")
            
            if not self.model:
                logger.error("âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ××ª×—×œ ××£ ××•×“×œ")
                print("âŒ [Expert Analysis] All models failed to initialize!")
        else:
            logger.warning("âš ï¸  ××¤×ª×— Google API ×œ× ××•×’×“×¨ - × ×™×ª×•×— ××•××—×™× ××•×©×‘×ª")
        
        self.is_configured = bool(self.api_key and self.model)
    
    def _build_transcript_text(self, segments: List[Dict], voice_map: Dict) -> Tuple[str, List[str]]:
        """
        Build readable transcript with speaker names resolved.
        Returns (transcript_text, list_of_speakers).
        """
        transcript_lines = []
        speakers_set = set()
        
        for seg in segments:
            speaker_id = seg.get("speaker", "×“×•×‘×¨ ×œ× ××–×•×”×”")
            # Replace speaker ID with name if known
            speaker_name = voice_map.get(speaker_id.lower(), speaker_id)
            if speaker_name == speaker_id:
                # Try without case sensitivity
                for key, value in voice_map.items():
                    if key.lower() == speaker_id.lower():
                        speaker_name = value
                        break
            
            speakers_set.add(speaker_name)
            text = seg.get("text", "")
            transcript_lines.append(f"**{speaker_name}**: {text}")
        
        return "\n".join(transcript_lines), list(speakers_set)
    
    async def detect_context(self, transcript_text: str) -> Dict[str, Any]:
        """
        Step 1: Use Gemini to categorize the conversation.
        
        Returns:
            {
                "primary_category": "Business|Parenting|Relationship|Leadership|General",
                "secondary_category": "..." (optional),
                "confidence": 0.0-1.0,
                "reasoning": "..."
            }
        """
        if not self.is_configured:
            return {"primary_category": "General", "confidence": 0.5, "reasoning": "Model not configured"}
        
        detection_prompt = f"""××ª×” ××•××—×” ×œ× ×™×ª×•×— ×©×™×—×•×ª. ×§×˜×’×¨ ××ª ×”×©×™×—×” ×”×‘××” ×œ××—×ª ××”×§×˜×’×•×¨×™×•×ª:

1. **Parenting** - ×©×™×—×•×ª ×¢×œ ×™×œ×“×™×, ×—×™× ×•×š, ××©×¤×—×”, ×’×‘×•×œ×•×ª ×‘×‘×™×ª
2. **Relationship** - ×©×™×—×•×ª ×¢×œ ×–×•×’×™×•×ª, ×™×—×¡×™×, ×¨×’×©×•×ª ×‘×™×Ÿ ×‘× ×™ ×–×•×’
3. **Business** - ×©×™×—×•×ª ×¢×œ ×¢×¡×§×™×, ×˜×›× ×•×œ×•×’×™×”, ×¤×¨×•×™×§×˜×™×, ×›×¡×£, ×¡×˜××¨×˜××¤
4. **Leadership** - ×©×™×—×•×ª ×¢×œ × ×™×”×•×œ ×¦×•×•×ª, ×ª×¨×‘×•×ª ××¨×’×•× ×™×ª, ×× ×˜×•×¨×™× ×’
5. **General** - ×©×™×—×•×ª ×›×œ×œ×™×•×ª ×©×œ× ××ª××™××•×ª ×œ××£ ×§×˜×’×•×¨×™×” ×¡×¤×¦×™×¤×™×ª

**×ª××œ×™×œ ×”×©×™×—×”:**
{transcript_text[:3000]}

**×”×—×–×¨ ×ª×©×•×‘×” ×‘×¤×•×¨××˜ JSON ×‘×œ×‘×“:**
{{
    "primary_category": "×§×˜×’×•×¨×™×” ×¨××©×™×ª",
    "secondary_category": "×§×˜×’×•×¨×™×” ××©× ×™×ª ××• null",
    "confidence": 0.8,
    "reasoning": "×”×¡×‘×¨ ×§×¦×¨"
}}
"""
        
        try:
            print(f"ğŸ” [Context Detection] Calling Gemini model: {self.model_name}")
            response = self.model.generate_content(
                detection_prompt,
                generation_config={
                    'temperature': 0.1,
                    'max_output_tokens': 500
                }
            )
            
            # Safe extraction of response.text
            try:
                response_text = response.text.strip() if response.text else ""
            except (ValueError, AttributeError) as text_err:
                print(f"   âš ï¸ response.text access failed: {text_err}")
                return self._fallback_context_detection(transcript_text)
            
            print(f"ğŸ” [Context Detection] Response received: {len(response_text)} chars")
            # Extract JSON from possible markdown code block
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            print(f"ğŸ¯ [Context Detection] Primary: {result.get('primary_category')}, Secondary: {result.get('secondary_category')}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Context detection failed: {e}")
            # Fallback to keyword-based detection
            return self._fallback_context_detection(transcript_text)
    
    def _fallback_context_detection(self, transcript_text: str) -> Dict[str, Any]:
        """Fallback to keyword-based detection if Gemini fails."""
        text_lower = transcript_text.lower()
        
        scores = {}
        for persona_key, persona in EXPERT_PERSONAS.items():
            if persona_key == "general":
                continue
            score = sum(1 for kw in persona["trigger_keywords"] if kw in text_lower)
            scores[persona_key] = score
        
        if not scores or max(scores.values()) == 0:
            return {"primary_category": "General", "confidence": 0.3, "reasoning": "No keywords matched"}
        
        # Map persona keys to categories
        category_map = {
            "parenting": "Parenting",
            "relationship": "Relationship",
            "strategy": "Business",
            "leadership": "Leadership"
        }
        
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        primary = sorted_scores[0]
        secondary = sorted_scores[1] if len(sorted_scores) > 1 and sorted_scores[1][1] > 0 else None
        
        return {
            "primary_category": category_map.get(primary[0], "General"),
            "secondary_category": category_map.get(secondary[0], None) if secondary else None,
            "confidence": min(1.0, primary[1] / 10),
            "reasoning": f"Keyword match: {primary[1]} for {primary[0]}"
        }
    
    def _get_personas_for_context(self, context: Dict[str, Any]) -> List[str]:
        """
        Map context categories to 1-2 persona keys.
        
        Routing:
        - Business/Tech â†’ McKinsey + Sinek (if leadership)
        - Parenting â†’ Adler
        - Relationship â†’ Esther Perel
        """
        category_to_persona = {
            "Parenting": "parenting",
            "Relationship": "relationship",
            "Business": "strategy",
            "Leadership": "leadership",
            "General": "general"
        }
        
        personas = []
        
        primary = context.get("primary_category", "General")
        if primary in category_to_persona:
            personas.append(category_to_persona[primary])
        
        # Add secondary persona if relevant and different
        secondary = context.get("secondary_category")
        if secondary and secondary in category_to_persona:
            secondary_persona = category_to_persona[secondary]
            if secondary_persona not in personas:
                # Only allow related combinations
                allowed_combos = [
                    ("strategy", "leadership"),
                    ("leadership", "strategy"),
                    ("parenting", "relationship"),
                    ("relationship", "parenting"),
                ]
                if (personas[0] if personas else None, secondary_persona) in allowed_combos:
                    personas.append(secondary_persona)
        
        # Ensure at least one persona
        if not personas:
            personas.append("general")
        
        print(f"ğŸ¯ [Persona Routing] Category '{primary}' -> Personas: {personas}")
        
        return personas[:2]  # Max 2 personas
    
    def build_expert_prompt(
        self, 
        persona_keys: List[str], 
        transcript_text: str, 
        speakers: List[str],
        context: Dict[str, Any]
    ) -> str:
        """
        Build deep analysis prompt with Expert Council structure.
        Balanced between depth and WhatsApp length limits.
        """
        # Get persona details
        personas = [EXPERT_PERSONAS.get(pk, EXPERT_PERSONAS["general"]) for pk in persona_keys]
        israel_time = get_israel_time()
        speakers_str = ", ".join(speakers) if speakers else "×œ× ×–×•×”×• ×“×•×‘×¨×™×"
        
        # Truncate transcript for prompt efficiency
        if len(transcript_text) > 3500:
            transcript_text = transcript_text[:3500] + "\n...(×§×•×¦×¨)"
        
        # Build persona section
        if len(personas) == 1:
            persona_section = f"**×”×¤×¨×¡×•× ×” ×©×œ×š:** {personas[0]['name']}\n**×’×™×©×”:** {personas[0]['tone']}"
        else:
            persona_section = f"**×”×¤×¨×¡×•× ×•×ª ×©×œ×š:** {personas[0]['name']} + {personas[1]['name']}"
        
        # Inject knowledge base context if available
        kb_block = ""
        try:
            kb_context = get_kb_context()
            if kb_context:
                kb_block = f"\n{kb_context}\n"
        except Exception:
            pass
        
        prompt = f"""××ª×” ×—×‘×¨ ×‘××•×¢×¦×ª ×”××•××—×™× ×©×œ "×”××•×— ×”×©× ×™".
{kb_block}
{persona_section}

**××©×ª×ª×¤×™×:** {speakers_str}
**×–××Ÿ:** {israel_time.strftime('%d/%m/%Y %H:%M')} (×©×¢×•×Ÿ ×™×©×¨××œ)
**×§×˜×’×•×¨×™×”:** {context.get('primary_category', '×›×œ×œ×™')}

**×ª××œ×™×œ:**
{transcript_text}

---

**×”× ×—×™×•×ª:**
1. ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“
2. ×”×©×ª××© ×‘×©××•×ª ×”×“×•×‘×¨×™× (×œ× "×“×•×‘×¨ 1")
3. ×›×©×™×© ××™×œ×™× ×‘×× ×’×œ×™×ª, ×”×ª×—×œ ××ª ×”××©×¤×˜ ×‘×¢×‘×¨×™×ª
4. **××’×‘×œ×ª ××•×¨×š: ×¡×”"×› ×¢×“ 800 ×ª×•×•×™×!**

---

**×¤×•×¨××˜ (×ª××¦×™×ª×™!):**

ğŸ­ **×¡× ×˜×™×× ×˜:** [×—×™×•×‘×™/××¢×•×¨×‘/××ª×•×—] - [××©×¤×˜ ×§×¦×¨]

ğŸ“‹ **×ª××¦×™×ª:**
â€¢ [××™ ×××¨ ××” - × ×§×•×“×” 1]
â€¢ [××™ ×××¨ ××” - × ×§×•×“×” 2]
â€¢ [×”×—×œ×˜×”/××¡×§× ×”]

ğŸ” **×ª×•×‘× ×ª {personas[0]['short_name']}:**
[2 ××©×¤×˜×™× ×¢× ×ª×•×‘× ×” ×¢××•×§×” - ××” ×§×•×¨×” ××ª×—×ª ×œ×¤× ×™ ×”×©×˜×—?]

âœ… **××©×™××•×ª:**
â€¢ *[×©×]*: [××©×™××”]
(×× ××™×Ÿ: "×œ× ×–×•×”×• ××©×™××•×ª")

ğŸ“ˆ **×§××™×–×Ÿ:**
âœ“ ×œ×©×™××•×¨: [×”×ª× ×”×’×•×ª ×—×™×•×‘×™×ª ×¡×¤×¦×™×¤×™×ª]
â†’ ×œ×©×™×¤×•×¨: [×”×–×“×× ×•×ª ×œ×¦××™×—×” + ×”××œ×¦×”]
"""
        return prompt
    
    async def analyze_transcript(
        self,
        segments: List[Dict],
        voice_map: Optional[Dict] = None,
        force_persona: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Perform full expert analysis on a transcript with Kaizen feedback.
        
        Flow:
        1. Build transcript text with resolved speaker names
        2. Detect context (category) using Gemini
        3. Assign 1-2 expert personas
        4. Run comprehensive analysis
        
        Args:
            segments: List of transcript segments
            voice_map: Optional mapping of speaker IDs to names
            force_persona: Optional - force a specific persona
            
        Returns:
            Dict with analysis results including Kaizen feedback
        """
        if not self.is_configured:
            return {
                "success": False,
                "error": "×©×™×¨×•×ª × ×™×ª×•×— ×”××•××—×™× ×œ× ××•×’×“×¨"
            }
        
        if not segments:
            return {
                "success": False,
                "error": "××™×Ÿ ×§×˜×¢×™ ×ª××œ×•×œ ×œ× ×™×ª×•×—"
            }
        
        voice_map = voice_map or {}
        
        # Step 1: Build transcript text
        transcript_text, speakers = self._build_transcript_text(segments, voice_map)
        print(f"ğŸ“ [Expert Analysis] Transcript: {len(transcript_text)} chars, {len(speakers)} speakers")
        
        # Step 2: Detect context
        print("ğŸ” [Expert Analysis] Step 1/3: Detecting context...")
        context = await self.detect_context(transcript_text)
        
        # Step 3: Assign personas
        if force_persona and force_persona in EXPERT_PERSONAS:
            persona_keys = [force_persona]
            print(f"ğŸ­ [Expert Analysis] Forced persona: {force_persona}")
        else:
            persona_keys = self._get_personas_for_context(context)
            print(f"ğŸ­ [Expert Analysis] Step 2/3: Assigned personas: {persona_keys}")
        
        personas = [EXPERT_PERSONAS[pk] for pk in persona_keys]
        persona_names = [p["name"] for p in personas]
        
        # Step 4: Build and run analysis with RETRY logic
        print(f"ğŸ§  [Expert Analysis] Step 3/3: Running deep analysis with model: {self.model_name}")
        prompt = self.build_expert_prompt(persona_keys, transcript_text, speakers, context)
        print(f"ğŸ“ [Expert Analysis] Prompt length: {len(prompt)} chars")
        
        analysis_text = ""
        max_retries = 3  # Increased to 3 attempts
        
        for attempt in range(max_retries):
            try:
                print(f"\n   {'='*40}")
                print(f"   ğŸ”„ ATTEMPT {attempt + 1}/{max_retries}")
                print(f"   {'='*40}")
                
                # Use progressively simpler prompts
                if attempt == 0:
                    current_prompt = prompt
                    print(f"   ğŸ“‹ Using: FULL expert prompt")
                elif attempt == 1:
                    current_prompt = self._build_fallback_prompt(transcript_text, speakers)
                    print(f"   ğŸ“‹ Using: FALLBACK prompt")
                else:
                    # Ultra-simple prompt for third attempt
                    current_prompt = self._build_minimal_prompt(transcript_text, speakers)
                    print(f"   ğŸ“‹ Using: MINIMAL prompt")
                
                print(f"   ğŸ“Š Model: {self.model_name}")
                print(f"   ğŸ“Š Prompt length: {len(current_prompt)} chars")
                print(f"   ğŸ“Š Prompt preview: {current_prompt[:200]}...")
                
                # Add safety settings to allow more content
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
                
                response = self.model.generate_content(
                    current_prompt,
                    generation_config={
                        'temperature': 0.3 if attempt > 0 else 0.4,
                        'max_output_tokens': 1000
                    },
                    safety_settings=safety_settings
                )
                
                # Debug: Check response structure
                print(f"   ğŸ“Š Response received")
                if hasattr(response, 'candidates') and response.candidates:
                    print(f"   ğŸ“Š Candidates: {len(response.candidates)}")
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        print(f"   ğŸ“Š Finish reason: {candidate.finish_reason}")
                    if candidate.content and candidate.content.parts:
                        print(f"   ğŸ“Š Parts: {len(candidate.content.parts)}")
                else:
                    print(f"   âš ï¸ No candidates!")
                    if hasattr(response, 'prompt_feedback'):
                        print(f"   ğŸ“Š Prompt feedback: {response.prompt_feedback}")
                
                # Safe extraction of response.text
                try:
                    analysis_text = response.text.strip() if response.text else ""
                    print(f"   âœ… Got text: {len(analysis_text)} chars")
                except (ValueError, AttributeError) as text_err:
                    print(f"   âš ï¸ response.text failed: {text_err}")
                    # Try to extract from candidates directly
                    if hasattr(response, 'candidates') and response.candidates:
                        try:
                            parts = response.candidates[0].content.parts
                            analysis_text = "".join(p.text for p in parts if hasattr(p, 'text'))
                            print(f"   âœ… Extracted from candidates: {len(analysis_text)} chars")
                        except Exception as extract_err:
                            print(f"   âš ï¸ Candidates extraction failed: {extract_err}")
                            analysis_text = ""
                    else:
                        analysis_text = ""
                
                print(f"   ğŸ“ Final text: {len(analysis_text)} chars")
                
                # Check for empty response
                if len(analysis_text.strip()) < 50:
                    print(f"   âš ï¸  Response too short ({len(analysis_text)} chars)")
                    print(f"   âš ï¸  Content: '{analysis_text[:100]}'" if analysis_text else "   âš ï¸  Content: EMPTY")
                    print(f"   ğŸ”„ Retrying with simpler prompt...")
                    continue
                
                # SUCCESS - break out of retry loop
                print(f"   âœ… SUCCESS! Got {len(analysis_text)} chars")
                break
                
            except Exception as e:
                print(f"   âŒ EXCEPTION in attempt {attempt + 1}: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
                if attempt == max_retries - 1:
                    error_msg = f"Exception: {type(e).__name__}: {str(e)[:50]}"
                    logger.error(f"âŒ [CRITICAL] × ×™×ª×•×— ××•××—×” × ×›×©×œ ×œ××—×¨ {max_retries} × ×¡×™×•× ×•×ª: {e}")
                    # Record error for system health reporting
                    try:
                        from app.services.architecture_audit_service import architecture_audit_service
                        architecture_audit_service.record_expert_error(error_msg)
                    except:
                        pass
        
        # Final validation
        if not analysis_text or len(analysis_text.strip()) < 50:
            print("âŒ [CRITICAL] Analysis returned EMPTY after all retries!")
            logger.error("âŒ [CRITICAL] Expert analysis returned empty text")
            error_msg = "× ×™×ª×•×— ×—×–×¨ ×¨×™×§ - ×‘×“×•×§ ××ª ×”××•×“×œ"
            
            # Record error for system health reporting
            try:
                from app.services.architecture_audit_service import architecture_audit_service
                architecture_audit_service.record_expert_error(error_msg)
            except:
                pass
            
            return {
                "success": False,
                "error": error_msg,
                "persona": " + ".join(persona_names),
                "model_used": self.model_name
            }
        
        print(f"âœ… [Expert Analysis] SUCCESS - {len(analysis_text)} chars")
        israel_time = get_israel_time()
        
        # Clear any recorded errors on success
        try:
            from app.services.architecture_audit_service import architecture_audit_service
            architecture_audit_service.clear_expert_error()
        except:
            pass
        
        return {
            "success": True,
            "persona": " + ".join(persona_names),
            "persona_keys": persona_keys,
            "context": context,
            "speakers": speakers,
            "raw_analysis": analysis_text,
            "timestamp": israel_time.isoformat(),
            "timestamp_display": israel_time.strftime('%d/%m/%Y %H:%M')
        }
    
    def _build_fallback_prompt(self, transcript_text: str, speakers: List[str]) -> str:
        """Simple fallback prompt when main prompt fails."""
        speakers_str = ", ".join(speakers) if speakers else "×“×•×‘×¨×™× ×œ× ×™×“×•×¢×™×"
        
        return f"""×¡×›× ××ª ×”×©×™×—×” ×”×‘××” ×‘×¢×‘×¨×™×ª.

**××©×ª×ª×¤×™×:** {speakers_str}

**×ª××œ×™×œ:**
{transcript_text[:3000]}

**×ª×©×•×‘×” ×‘×¤×•×¨××˜ ×”×‘×:**

ğŸ¯ ×¡× ×˜×™×× ×˜: [×—×™×•×‘×™/×©×œ×™×œ×™/××¢×•×¨×‘]

ğŸ“‹ ×ª××¦×™×ª:
â€¢ [××™ ×××¨ ××” - × ×§×•×“×” 1]
â€¢ [××™ ×××¨ ××” - × ×§×•×“×” 2]

âœ… ××©×™××•×ª:
â€¢ [×©×]: [××©×™××”]

ğŸ“ˆ ×§××™×–×Ÿ:
âœ“ ×œ×©×™××•×¨: [× ×§×•×“×” ×—×™×•×‘×™×ª]
â†’ ×œ×©×™×¤×•×¨: [×”×–×“×× ×•×ª ×œ×¦××™×—×”]
"""
    
    def _build_minimal_prompt(self, transcript_text: str, speakers: List[str]) -> str:
        """Ultra-simple prompt for last resort attempt."""
        speakers_str = ", ".join(speakers) if speakers else "×“×•×‘×¨×™×"
        
        # Very short transcript sample
        short_transcript = transcript_text[:1500] if len(transcript_text) > 1500 else transcript_text
        
        return f"""×¡×›× ×‘×§×¦×¨×” ××ª ×”×©×™×—×” ×”×–××ª ×‘×¢×‘×¨×™×ª:

{short_transcript}

××©×ª×ª×¤×™×: {speakers_str}

×›×ª×•×‘ 3-4 ××©×¤×˜×™× ×§×¦×¨×™× ×©××¡×›××™× ××ª ×¢×™×§×¨ ×”×©×™×—×”.
"""
    
    def analyze_audio_direct(self, audio_path: str) -> Dict[str, Any]:
        """
        Direct audio analysis using the proven SYSTEM_INSTRUCTION prompt.
        
        This bypasses the multi-step transcript analysis and sends audio
        directly to Gemini with a comprehensive expert prompt.
        This is the same approach used in process_meetings.py which works reliably.
        
        Args:
            audio_path: Path to the audio file
            
        Returns:
            Dict with success status and analysis text
        """
        import time
        from pathlib import Path
        
        if not self.is_configured:
            return {
                "success": False,
                "error": "×©×™×¨×•×ª ×”× ×™×ª×•×— ×œ× ××•×’×“×¨",
                "source": "direct"
            }
        
        audio_file = Path(audio_path)
        if not audio_file.exists():
            return {
                "success": False,
                "error": f"×§×•×‘×¥ ×”××•×“×™×• ×œ× × ××¦×: {audio_path}",
                "source": "direct"
            }
        
        print(f"ğŸ™ï¸ [Direct Analysis] Starting direct audio analysis...")
        print(f"   ğŸ“ File: {audio_file.name}")
        print(f"   ğŸ“ Size: {audio_file.stat().st_size / 1024:.1f} KB")
        
        try:
            # Determine MIME type from file extension
            mime_type_map = {
                '.mp3': 'audio/mpeg',
                '.wav': 'audio/wav',
                '.wave': 'audio/wav',
                '.m4a': 'audio/mp4',
                '.aac': 'audio/aac',
                '.ogg': 'audio/ogg',
                '.flac': 'audio/flac',
                '.mp4': 'audio/mp4',
                '.opus': 'audio/opus',
            }
            
            file_ext = audio_file.suffix.lower()
            mime_type = mime_type_map.get(file_ext, 'audio/mpeg')
            print(f"   ğŸ“‹ MIME type: {mime_type}")
            
            # Upload file to Gemini
            print(f"   ğŸ“¤ Uploading to Gemini...")
            file_ref = genai.upload_file(
                path=str(audio_path),
                display_name=audio_file.name,
                mime_type=mime_type
            )
            
            # Wait for file to be processed
            print(f"   â³ Waiting for Gemini processing...")
            max_wait = 120  # 2 minutes
            start_time = time.time()
            
            while time.time() - start_time < max_wait:
                file_ref = genai.get_file(file_ref.name)
                state = file_ref.state.name if hasattr(file_ref.state, 'name') else str(file_ref.state)
                
                if state == "ACTIVE":
                    print(f"   âœ… File processing complete")
                    break
                elif state == "FAILED":
                    return {
                        "success": False,
                        "error": f"Gemini failed to process file: {file_ref.name}",
                        "source": "direct"
                    }
                
                time.sleep(2)
            else:
                return {
                    "success": False,
                    "error": "Timeout waiting for Gemini to process audio",
                    "source": "direct"
                }
            
            # Generate content with expert prompt + Knowledge Base
            print(f"   ğŸ§  Running expert analysis...")
            
            # Inject personal knowledge base into system instruction
            system_instruction = DIRECT_AUDIO_SYSTEM_INSTRUCTION
            kb_context = get_kb_context()
            if kb_context:
                system_instruction += "\n" + kb_context
                print(f"   ğŸ“š Knowledge Base injected ({len(kb_context)} chars)")
            
            contents = [
                system_instruction,
                file_ref
            ]
            
            # Use safety settings to prevent blocking
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
            
            response = self.model.generate_content(
                contents,
                generation_config={
                    'temperature': 0.7,
                    'top_p': 0.95,
                    'top_k': 40,
                    'max_output_tokens': 2000,
                },
                safety_settings=safety_settings
            )
            
            # Extract text safely
            analysis_text = ""
            try:
                analysis_text = response.text.strip() if response.text else ""
            except (ValueError, AttributeError):
                # Try direct extraction from candidates
                if response.candidates and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            if hasattr(part, 'text'):
                                analysis_text += part.text
            
            # Clean up uploaded file
            try:
                genai.delete_file(file_ref.name)
                print(f"   ğŸ—‘ï¸ Deleted temp file from Gemini")
            except Exception as del_err:
                print(f"   âš ï¸ Could not delete Gemini file: {del_err}")
            
            if not analysis_text or len(analysis_text.strip()) < 50:
                return {
                    "success": False,
                    "error": "Gemini returned empty analysis",
                    "source": "direct"
                }
            
            israel_time = get_israel_time()
            print(f"   âœ… Direct analysis complete: {len(analysis_text)} chars")
            
            return {
                "success": True,
                "raw_analysis": analysis_text,
                "source": "direct",
                "timestamp": israel_time.isoformat(),
                "timestamp_display": israel_time.strftime('%d/%m/%Y %H:%M')
            }
            
        except Exception as e:
            print(f"   âŒ Direct analysis error: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            
            # Record error for health reporting
            try:
                from app.services.architecture_audit_service import architecture_audit_service
                architecture_audit_service.record_expert_error(f"Direct: {str(e)[:50]}")
            except:
                pass
            
            return {
                "success": False,
                "error": f"{type(e).__name__}: {str(e)[:100]}",
                "source": "direct"
            }
    
    def format_for_whatsapp(self, analysis_result: Dict, include_header: bool = True) -> str:
        """
        Format the expert analysis for WhatsApp message.
        
        STRICT: Total message must be under 1600 characters.
        
        Args:
            analysis_result: Result from analyze_transcript
            include_header: Whether to include the decorative header
            
        Returns:
            Formatted WhatsApp message string (max 1600 chars)
        """
        if not analysis_result.get("success"):
            error = analysis_result.get('error', '×©×’×™××” ×œ× ×™×“×•×¢×”')
            return f"âš ï¸ ×œ× ×”×¦×œ×—×ª×™ ×œ×‘×¦×¢ × ×™×ª×•×— ××•××—×”: {error}"
        
        raw = analysis_result.get("raw_analysis", "")
        source = analysis_result.get("source", "transcript")
        
        # Debug logging
        print(f"ğŸ“Š [format_for_whatsapp] Source: {source}, Raw analysis: {len(raw)} chars")
        if not raw:
            print("   âš ï¸  WARNING: raw_analysis is EMPTY!")
        
        # Build message - Direct/Combined analysis already has full formatting
        message = ""
        
        if source in ("direct", "combined"):
            # Direct audio analysis or Combined prompt already includes expert header (ğŸ§  ×”×›×•×‘×¢ ×©× ×‘×—×¨)
            # No need to add extra header - use raw content as-is
            message = raw
        else:
            # Transcript-based analysis - add minimal header
            persona = analysis_result.get("persona", "×¢×•×–×¨ ××™×©×™")
            context = analysis_result.get("context", {})
            
            if include_header:
                category = context.get('primary_category', '×›×œ×œ×™')
                message += f"ğŸ§  *{persona}* | {category}\n\n"
            
            message += raw
        
        # STRICT: Enforce 1200 char limit to prevent truncation
        MAX_LENGTH = 1200
        if len(message) > MAX_LENGTH:
            print(f"   âš ï¸ Message too long ({len(message)} chars), trimming to {MAX_LENGTH}")
            
            # Try to include Kaizen at the end
            kaizen_start = message.find("ğŸ“ˆ ×§××™×–×Ÿ")
            
            if kaizen_start > 0 and kaizen_start < MAX_LENGTH - 200:
                # Kaizen fits within limit - keep from Kaizen onwards
                before_kaizen = message[:kaizen_start].strip()
                kaizen_section = message[kaizen_start:]
                
                # Trim before_kaizen to fit
                available = MAX_LENGTH - len(kaizen_section) - 50
                if len(before_kaizen) > available:
                    # Find last complete line
                    before_kaizen = before_kaizen[:available]
                    last_newline = before_kaizen.rfind('\n')
                    if last_newline > available * 0.5:
                        before_kaizen = before_kaizen[:last_newline]
                
                message = before_kaizen + "\n\n" + kaizen_section
            else:
                # Kaizen too far or not found - just truncate
                message = message[:MAX_LENGTH - 30]
                last_newline = message.rfind('\n')
                if last_newline > MAX_LENGTH * 0.7:
                    message = message[:last_newline]
                message += "\n\n_(×§×•×¦×¨)_"
        
        print(f"ğŸ“Š [format_for_whatsapp] Final message: {len(message)} chars")
        return message
    
    def save_analysis_to_drive(
        self,
        analysis_result: Dict,
        transcript_file_id: str,
        drive_service
    ) -> Optional[str]:
        """
        Save the expert analysis as a companion file to the transcript.
        This enables retroactive updates when speakers are identified.
        
        Returns:
            File ID of saved analysis, or None if failed
        """
        if not analysis_result.get("success"):
            return None
        
        try:
            # Build analysis document
            analysis_doc = {
                "persona": analysis_result.get("persona"),
                "persona_keys": analysis_result.get("persona_keys"),
                "context": analysis_result.get("context"),
                "speakers": analysis_result.get("speakers"),
                "analysis": analysis_result.get("raw_analysis"),
                "timestamp": analysis_result.get("timestamp"),
                "transcript_file_id": transcript_file_id
            }
            
            # Save via drive service if available
            # This would be implemented in drive_memory_service
            # For now, the analysis is included in the main transcript save
            return None
            
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘×©××™×¨×ª × ×™×ª×•×—: {e}")
            return None


# Singleton instance
expert_analysis_service = ExpertAnalysisService()
