"""
Expert Analysis Service - Council of Experts for Meeting Analysis

This service applies expert personas to analyze transcribed conversations:
- Michal Dalyot / Adler Institute (Parenting/Boundaries)
- Esther Perel (Relationships/Communication)
- McKinsey & Co. / Israeli Tech (Business/Strategy)
- Simon Sinek (Leadership/The Why)

Flow:
1. Context Detection - Gemini categorizes the conversation
2. Expert Assignment - 1-2 most relevant personas assigned
3. Deep Analysis - Who said what, sentiment, insights
4. Mandatory Kaizen - Preserve/Improve feedback

Each persona provides:
- Sentiment analysis
- Executive summary (deep attribution)
- Expert insights from assigned persona(s)
- Action items (assigned to specific speakers)
- Mandatory Kaizen Feedback (×œ×©×™××•×¨ / ×œ×©×™×¤×•×¨)
"""

import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta

import google.generativeai as genai

from app.core.config import settings

logger = logging.getLogger(__name__)

# Israel timezone offset (UTC+2 winter, UTC+3 summer)
ISRAEL_TZ_OFFSET = timedelta(hours=2)


def get_israel_time() -> datetime:
    """Get current time in Israel timezone."""
    utc_now = datetime.now(timezone.utc)
    return utc_now + ISRAEL_TZ_OFFSET


# Council of Experts - Persona Definitions
EXPERT_PERSONAS = {
    "parenting": {
        "name": "××™×›×œ ×“×œ×™×•×ª / ××›×•×Ÿ ××“×œ×¨ (×”×•×¨×•×ª ×•×’×‘×•×œ×•×ª)",
        "short_name": "××›×•×Ÿ ××“×œ×¨",
        "category": "Parenting",
        "trigger_keywords": [
            "×™×œ×“×™×", "×‘× ×™×", "×‘× ×•×ª", "×‘×™×ª", "×—×™× ×•×š", "××˜×œ×•×ª", "×©×™×¢×•×¨×™ ×‘×™×ª",
            "kids", "children", "×”×•×¨×™×", "××©×¤×—×”", "×’×‘×•×œ×•×ª", "×›×œ×œ×™×",
            "×™×œ×“", "×™×œ×“×”", "×‘×Ÿ", "×‘×ª", "×”×ª× ×”×’×•×ª", "××©××¢×ª", "×”×•×¨×•×ª",
            "×’×Ÿ", "×‘×™×ª ×¡×¤×¨", "××•×¨×”", "×˜×™×¤×•×œ", "×—×•××”", "×œ×™×œ×”", "×©×™× ×”",
            "××•×›×œ", "×¡×œ×•×œ×¨", "××¡×›×™×", "×™×—×“", "××—×™×", "××—×™×•×ª"
        ],
        "focus": """
×’×™×©×ª ××“×œ×¨ ×œ×”×•×¨×•×ª ×“××•×§×¨×˜×™×ª:
- ×¢×™×“×•×“ ×‘××§×•× ×©×‘×— (Encouragement vs Praise)
- ×ª×•×¦××•×ª ×˜×‘×¢×™×•×ª ×•×”×’×™×•× ×™×•×ª (Natural & Logical Consequences)
- ×©×™×ª×•×£ ×¤×¢×•×œ×” ×•××—×¨×™×•×ª ××©×•×ª×¤×ª
- ×”×™×× ×¢×•×ª ××××‘×§×™ ×›×•×—
- ×›×‘×•×“ ×”×“×“×™ ×•×’×‘×•×œ×•×ª ×‘×¨×•×¨×™×
- ××ª×Ÿ ×‘×—×™×¨×•×ª ×‘××¡×’×¨×ª ×’×‘×•×œ×•×ª
- ×–×™×”×•×™ "××˜×¨×•×ª ××•×˜×¢×•×ª" ×‘×”×ª× ×”×’×•×ª
""",
        "tone": "×ª×•××š, ×¤×¨×§×˜×™, ×—×™× ×•×›×™, ×œ×œ× ×©×™×¤×•×˜×™×•×ª",
        "key_questions": [
            "×”×× ×”×©×™×—×” ××©×ª××©×ª ×‘×©×¤×” ××¢×•×“×“×ª ××• ×‘×‘×™×§×•×¨×ª?",
            "×”×× ×™×© ×××‘×§×™ ×›×•×— × ×¡×ª×¨×™×?",
            "×”×× ×”×’×‘×•×œ×•×ª ×©×”×•×’×“×¨×• ×‘×¨×•×¨×™× ×•×˜×‘×¢×™×™×?"
        ]
    },
    "relationship": {
        "name": "××¡×ª×¨ ×¤×¨×œ (×™×—×¡×™× ×•×ª×§×©×•×¨×ª)",
        "short_name": "××¡×ª×¨ ×¤×¨×œ",
        "category": "Relationship",
        "trigger_keywords": [
            "×‘×¢×œ", "××™×©×”", "×–×•×’×™×•×ª", "×¨×’×©×•×ª", "××”×‘×”", "relationship", "partner",
            "×¢×¨×Ÿ", "×–×•×’", "×›×¢×¡", "×ª×¡×›×•×œ", "××™× ×˜×™××™×•×ª", "×§×©×¨", "×—×™×‘×•×¨",
            "×¨×™×‘", "×•×™×›×•×—", "×œ× ××‘×™×Ÿ", "×œ× ××‘×™× ×”", "××¨×—×§", "×§×¨×‘×”",
            "×—×•×¤×©×”", "×–×•×’×™", "×‘×™×—×“", "× ×™×©×•××™×Ÿ", "×©×•×ª×¤×•×ª", "×ª××™×›×”"
        ],
        "focus": """
×’×™×©×ª ××¡×ª×¨ ×¤×¨×œ ×œ×–×•×’×™×•×ª:
- ××™× ×˜×œ×™×’× ×¦×™×” ×¨×’×©×™×ª ×¢××•×§×”
- ××™×–×•×Ÿ ×‘×™×Ÿ ×‘×™×˜×—×•×Ÿ (Security) ×œ×—×•×¤×© (Freedom)
- ×”×§×©×‘×” ×œ"×œ× × ×××¨" - ××” ×‘×™×Ÿ ×”×©×•×¨×•×ª
- ×”×‘× ×ª ×“×™× ××™×§×•×ª ×›×•×— × ×¡×ª×¨×•×ª
- ×¤×™×©×•×¨ ×‘×™×Ÿ ×ª×©×•×§×” ×œ×—×™×™ ×™×•×-×™×•×
- ×–×™×”×•×™ ×“×¤×•×¡×™× ×—×•×–×¨×™× ×‘×ª×§×©×•×¨×ª
- ×”×¢×¨×›×ª "×¡×’× ×•× ×•×ª ×”×ª×§×©×¨×•×ª" (Attachment Styles)
""",
        "tone": "×××¤×ª×™, ×¢××•×§, ×ª×•×‘× ×ª×™, ×œ×œ× ×©×™×¤×•×˜",
        "key_questions": [
            "××” ×œ× × ×××¨ ×‘×©×™×—×”?",
            "××™×–×” ×¨×’×© × ×¡×ª×¨ ××ª×—×ª ×œ×¤× ×™ ×”×©×˜×—?",
            "×”×× ×™×© ×“×¤×•×¡ ×—×•×–×¨ ×©×¨××•×™ ×œ×©×™× ×œ×‘ ××œ×™×•?"
        ]
    },
    "strategy": {
        "name": "××§×™× ×–×™ ×•×˜×§ ×™×©×¨××œ×™ (××¡×˜×¨×˜×’×™×” ×•×¢×¡×§×™×)",
        "short_name": "××§×™× ×–×™ + Tech",
        "category": "Business",
        "trigger_keywords": [
            "×¢×¡×§", "××•×¦×¨", "×œ×§×•×—×•×ª", "××¡×˜×¨×˜×’×™×”", "roadmap", "MVP", "startup",
            "product", "×›×¡×£", "×ª×§×¦×™×‘", "×”×›× ×¡×•×ª", "××›×™×¨×•×ª", "×©×•×§", "××ª×—×¨×™×",
            "×¤×™×¦'×¨", "×¤×™×ª×•×—", "tech", "×§×•×“", "××©×§×™×¢×™×", "×’×™×•×¡", "×—×‘×¨×”",
            "×¢×¡×§×™×", "×¤×¨×•×™×§×˜", "×“×“×œ×™×™×Ÿ", "×™×¢×“×™×", "KPI", "OKR", "×‘×™×¦×•×¢×™×",
            "×ª×›× ×•×Ÿ", "××¤×œ×™×§×¦×™×”", "××ª×¨", "×©×™×¨×•×ª", "B2B", "B2C", "SaaS"
        ],
        "focus": """
×’×™×©×ª ××§×™× ×–×™ + High-Tech ×™×©×¨××œ×™:
- ××‘× ×” MECE (Mutually Exclusive, Collectively Exhaustive)
- × ×™×ª×•×— ××‘×•×¡×¡ × ×ª×•× ×™× (Data-Driven)
- ×¡×§×™×™×œ×‘×™×œ×™×•×ª ×•×™×¢×™×œ×•×ª
- ×—×©×™×‘×ª Agile/Lean Startup
- MVP ×•××™×˜×¨×¦×™×•×ª ××”×™×¨×•×ª
- ×”×’×“×¨×ª KPIs ×‘×¨×•×¨×™×
- ×ª×¢×“×•×£ ××›×–×¨×™ (Ruthless Prioritization)
- "First Principles Thinking"
""",
        "tone": "×—×“, ××§×¦×•×¢×™, ×××•×§×“ ×¤×¢×•×œ×”, ×—×•×ª×š ×œ×¢× ×™×™×Ÿ",
        "key_questions": [
            "××” ×”-ROI ×”×¦×¤×•×™?",
            "××” ×”×¦×¢×“ ×”×‘× ×”×§×˜×Ÿ ×‘×™×•×ª×¨ ×©××‘×™× ×¢×¨×š?",
            "××” ×”-bottleneck ×”×××™×ª×™?"
        ]
    },
    "leadership": {
        "name": "×¡×™×™××•×Ÿ ×¡×™× ×§ (×× ×”×™×’×•×ª ×•×”×©×¨××”)",
        "short_name": "×¡×™×™××•×Ÿ ×¡×™× ×§",
        "category": "Leadership",
        "trigger_keywords": [
            "×¦×•×•×ª", "×¢×•×‘×“×™×", "×× ×”×œ", "×—×‘×¨×”", "×ª×¨×‘×•×ª", "hiring", "mentoring",
            "culture", "×× ×”×™×’×•×ª", "×”×©×¨××”", "×¢×¨×›×™×", "×—×–×•×Ÿ", "××©××¢×•×ª",
            "××•×˜×™×‘×¦×™×”", "×’×™×•×¡", "×¤×™×˜×•×¨×™×", "×‘×™×¦×•×¢×™×", "××©×•×‘", "feedback",
            "1on1", "××—×“ ×¢×œ ××—×“", "×¦××™×—×”", "×§×¨×™×™×¨×”", "×¤×•×˜× ×¦×™××œ"
        ],
        "focus": """
×’×™×©×ª ×¡×™×™××•×Ÿ ×¡×™× ×§ ×œ×× ×”×™×’×•×ª:
- Start with Why - ×ª×ª×—×™×œ ××”"×œ××”"
- The Infinite Game - ××©×—×§ ××™× ×¡×•×¤×™, ×œ× × ×™×¦×—×•×Ÿ ×—×“ ×¤×¢××™
- Circle of Safety - ×™×¦×™×¨×ª ××¢×’×œ ×‘×™×˜×—×•×Ÿ ×œ×¦×•×•×ª
- Leaders Eat Last - ×× ×”×™×’ ×“×•××’ ×§×•×“× ×œ××—×¨×™×
- ×× ×”×™×’×•×ª ××©×¨×ª×ª (Servant Leadership)
- ×‘× ×™×™×ª ×××•×Ÿ ×œ×˜×•×•×— ××¨×•×š
- "Optimism fueled by reality"
""",
        "tone": "××¢×•×¨×¨ ×”×©×¨××”, ×××•×§×“ ×‘×× ×©×™×, ×—×–×•× ×™, ×× ×•×©×™",
        "key_questions": [
            "××” ×”'×œ××”' ×××—×•×¨×™ ×”×”×—×œ×˜×”?",
            "×”×× ×–×” ×‘×•× ×” ×××•×Ÿ ×œ×˜×•×•×— ××¨×•×š?",
            "×”×× ×”×× ×”×™×’×•×ª ×›××Ÿ ××©×¨×ª×ª ××• ×©×•×œ×˜×ª?"
        ]
    },
    "general": {
        "name": "×¢×•×–×¨ ××™×©×™ ×—×›×",
        "short_name": "×¢×•×–×¨ ××™×©×™",
        "category": "General",
        "trigger_keywords": [],
        "focus": "×¡×™×›×•× ×‘×¨×•×¨ ×•×ª××¦×™×ª×™ ×¢× ×ª×•×‘× ×•×ª ×¤×¨×§×˜×™×•×ª ×•××§×©×Ÿ ××™×™×˜××¡",
        "tone": "×™×©×™×¨, ×©×™××•×©×™, ×¤×¨×§×˜×™",
        "key_questions": ["××” ×”× ×§×•×“×•×ª ×”×¢×™×§×¨×™×•×ª?", "××” ×”×¦×¢×“ ×”×‘×?"]
    }
}


class ExpertAnalysisService:
    """
    Council of Experts Analysis System for meeting transcripts.
    
    Flow:
    1. Context Detection - Ask Gemini to categorize the transcript
    2. Expert Assignment - Select 1-2 relevant personas
    3. Deep Analysis - Full analysis with attribution
    4. Mandatory Kaizen - Preserve/Improve feedback
    """
    
    def __init__(self):
        self.api_key = settings.google_api_key
        if self.api_key:
            genai.configure(api_key=self.api_key)
            try:
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                logger.info("âœ… ×©×™×¨×•×ª × ×™×ª×•×— ×”××•××—×™× ××•×ª×—×œ ×¢× Gemini 1.5 Flash")
            except Exception as e:
                logger.error(f"âŒ ×©×’×™××” ×‘××ª×—×•×œ ×”××•×“×œ: {e}")
                self.model = None
        else:
            self.model = None
            logger.warning("âš ï¸  ××¤×ª×— Google API ×œ× ××•×’×“×¨ - × ×™×ª×•×— ××•××—×™× ××•×©×‘×ª")
        
        self.is_configured = bool(self.api_key and self.model)
    
    def _build_transcript_text(self, segments: List[Dict], voice_map: Dict) -> Tuple[str, List[str]]:
        """
        Build readable transcript with speaker names resolved.
        Returns (transcript_text, list_of_speakers).
        """
        transcript_lines = []
        speakers_set = set()
        
        for seg in segments:
            speaker_id = seg.get("speaker", "×“×•×‘×¨ ×œ× ××–×•×”×”")
            # Replace speaker ID with name if known
            speaker_name = voice_map.get(speaker_id.lower(), speaker_id)
            if speaker_name == speaker_id:
                # Try without case sensitivity
                for key, value in voice_map.items():
                    if key.lower() == speaker_id.lower():
                        speaker_name = value
                        break
            
            speakers_set.add(speaker_name)
            text = seg.get("text", "")
            transcript_lines.append(f"**{speaker_name}**: {text}")
        
        return "\n".join(transcript_lines), list(speakers_set)
    
    async def detect_context(self, transcript_text: str) -> Dict[str, Any]:
        """
        Step 1: Use Gemini to categorize the conversation.
        
        Returns:
            {
                "primary_category": "Business|Parenting|Relationship|Leadership|General",
                "secondary_category": "..." (optional),
                "confidence": 0.0-1.0,
                "reasoning": "..."
            }
        """
        if not self.is_configured:
            return {"primary_category": "General", "confidence": 0.5, "reasoning": "Model not configured"}
        
        detection_prompt = f"""××ª×” ××•××—×” ×œ× ×™×ª×•×— ×©×™×—×•×ª. ×§×˜×’×¨ ××ª ×”×©×™×—×” ×”×‘××” ×œ××—×ª ××”×§×˜×’×•×¨×™×•×ª:

1. **Parenting** - ×©×™×—×•×ª ×¢×œ ×™×œ×“×™×, ×—×™× ×•×š, ××©×¤×—×”, ×’×‘×•×œ×•×ª ×‘×‘×™×ª
2. **Relationship** - ×©×™×—×•×ª ×¢×œ ×–×•×’×™×•×ª, ×™×—×¡×™×, ×¨×’×©×•×ª ×‘×™×Ÿ ×‘× ×™ ×–×•×’
3. **Business** - ×©×™×—×•×ª ×¢×œ ×¢×¡×§×™×, ×˜×›× ×•×œ×•×’×™×”, ×¤×¨×•×™×§×˜×™×, ×›×¡×£, ×¡×˜××¨×˜××¤
4. **Leadership** - ×©×™×—×•×ª ×¢×œ × ×™×”×•×œ ×¦×•×•×ª, ×ª×¨×‘×•×ª ××¨×’×•× ×™×ª, ×× ×˜×•×¨×™× ×’
5. **General** - ×©×™×—×•×ª ×›×œ×œ×™×•×ª ×©×œ× ××ª××™××•×ª ×œ××£ ×§×˜×’×•×¨×™×” ×¡×¤×¦×™×¤×™×ª

**×ª××œ×™×œ ×”×©×™×—×”:**
{transcript_text[:3000]}

**×”×—×–×¨ ×ª×©×•×‘×” ×‘×¤×•×¨××˜ JSON ×‘×œ×‘×“:**
{{
    "primary_category": "×§×˜×’×•×¨×™×” ×¨××©×™×ª",
    "secondary_category": "×§×˜×’×•×¨×™×” ××©× ×™×ª ××• null",
    "confidence": 0.8,
    "reasoning": "×”×¡×‘×¨ ×§×¦×¨"
}}
"""
        
        try:
            response = self.model.generate_content(
                detection_prompt,
                generation_config={
                    'temperature': 0.1,
                    'max_output_tokens': 500
                }
            )
            
            # Parse JSON response
            response_text = response.text.strip()
            # Extract JSON from possible markdown code block
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            print(f"ğŸ¯ [Context Detection] Primary: {result.get('primary_category')}, Secondary: {result.get('secondary_category')}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Context detection failed: {e}")
            # Fallback to keyword-based detection
            return self._fallback_context_detection(transcript_text)
    
    def _fallback_context_detection(self, transcript_text: str) -> Dict[str, Any]:
        """Fallback to keyword-based detection if Gemini fails."""
        text_lower = transcript_text.lower()
        
        scores = {}
        for persona_key, persona in EXPERT_PERSONAS.items():
            if persona_key == "general":
                continue
            score = sum(1 for kw in persona["trigger_keywords"] if kw in text_lower)
            scores[persona_key] = score
        
        if not scores or max(scores.values()) == 0:
            return {"primary_category": "General", "confidence": 0.3, "reasoning": "No keywords matched"}
        
        # Map persona keys to categories
        category_map = {
            "parenting": "Parenting",
            "relationship": "Relationship",
            "strategy": "Business",
            "leadership": "Leadership"
        }
        
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        primary = sorted_scores[0]
        secondary = sorted_scores[1] if len(sorted_scores) > 1 and sorted_scores[1][1] > 0 else None
        
        return {
            "primary_category": category_map.get(primary[0], "General"),
            "secondary_category": category_map.get(secondary[0], None) if secondary else None,
            "confidence": min(1.0, primary[1] / 10),
            "reasoning": f"Keyword match: {primary[1]} for {primary[0]}"
        }
    
    def _get_personas_for_context(self, context: Dict[str, Any]) -> List[str]:
        """
        Map context categories to persona keys.
        Returns 1-2 persona keys.
        """
        category_to_persona = {
            "Parenting": "parenting",
            "Relationship": "relationship",
            "Business": "strategy",
            "Leadership": "leadership",
            "General": "general"
        }
        
        personas = []
        
        primary = context.get("primary_category", "General")
        if primary in category_to_persona:
            personas.append(category_to_persona[primary])
        
        secondary = context.get("secondary_category")
        if secondary and secondary in category_to_persona:
            secondary_persona = category_to_persona[secondary]
            if secondary_persona not in personas:
                personas.append(secondary_persona)
        
        # Ensure at least one persona
        if not personas:
            personas.append("general")
        
        return personas[:2]  # Max 2 personas
    
    def build_expert_prompt(
        self, 
        persona_keys: List[str], 
        transcript_text: str, 
        speakers: List[str],
        context: Dict[str, Any]
    ) -> str:
        """
        Build the comprehensive analysis prompt with:
        - Multi-persona insights
        - Deep attribution (who said what)
        - Mandatory Kaizen feedback
        """
        # Get persona details
        personas = [EXPERT_PERSONAS.get(pk, EXPERT_PERSONAS["general"]) for pk in persona_keys]
        israel_time = get_israel_time()
        
        # Build persona section
        if len(personas) == 1:
            persona_section = f"""**×”×¤×¨×¡×•× ×” ×©×œ×š:** {personas[0]['name']}

**×”×’×™×©×” ×•×”××ª×•×“×•×œ×•×’×™×”:**
{personas[0]['focus']}

**×”×˜×•×Ÿ:** {personas[0]['tone']}

**×©××œ×•×ª ××¤×ª×— ×œ× ×™×ª×•×—:**
{chr(10).join('- ' + q for q in personas[0].get('key_questions', []))}
"""
        else:
            persona_section = f"""**×”×¤×¨×¡×•× ×•×ª ×©×œ×š (×©×œ×‘ × ×§×•×“×•×ª ××‘×˜ ××©×ª×™×”×Ÿ):**

ğŸ”¹ **{personas[0]['name']}:**
{personas[0]['focus']}
×˜×•×Ÿ: {personas[0]['tone']}

ğŸ”¹ **{personas[1]['name']}:**
{personas[1]['focus']}
×˜×•×Ÿ: {personas[1]['tone']}
"""
        
        # Build speakers list
        speakers_str = ", ".join(speakers) if speakers else "×œ× ×–×•×”×• ×“×•×‘×¨×™×"
        
        prompt = f"""××ª×” ×—×‘×¨ ×‘××•×¢×¦×ª ×”××•××—×™× ×©×œ "×”××•×— ×”×©× ×™" (Second Brain).
××˜×¨×ª×š ×œ×¡×¤×§ × ×™×ª×•×— ×¢××•×§ ×•××¤×•×¨×˜ ×©×œ ×”×©×™×—×”, ×¢× ×“×’×© ×¢×œ **××™ ×××¨ ××”** (attribution).

{persona_section}

**××©×ª×ª×¤×™× ×‘×©×™×—×”:** {speakers_str}
**×–××Ÿ ×”× ×™×ª×•×—:** {israel_time.strftime('%d/%m/%Y %H:%M')} (×©×¢×•×Ÿ ×™×©×¨××œ)
**×§×˜×’×•×¨×™×” ××–×•×”×”:** {context.get('primary_category', '×›×œ×œ×™')}

---

**×ª××œ×™×œ ×”×©×™×—×”:**
{transcript_text}

---

**×”× ×—×™×•×ª ×§×¨×™×˜×™×•×ª:**
1. ğŸ‡®ğŸ‡± **×›×ª×•×‘ ×‘×¢×‘×¨×™×ª ×‘×œ×‘×“**
2. ğŸ·ï¸ **×”×©×ª××© ×‘×©××•×ª ×”×××™×ª×™×™× ×©×œ ×”×“×•×‘×¨×™×** - ×œ× "×“×•×‘×¨ 1" ××• "Speaker 2"
3. ğŸ“Œ **×¦×™×™×Ÿ ××™ ×××¨ ××”** - ×›×œ ×ª×•×‘× ×” ×—×©×•×‘×” ×¦×¨×™×›×” ×œ×›×œ×•×œ attribution
4. ğŸ¯ **×”×™×” ×¡×¤×¦×™×¤×™ ×•×¤×¨×§×˜×™** - ×ª×•×‘× ×•×ª ×©××¤×©×¨ ×œ×™×™×©× ×”×™×•×
5. ğŸ“ **RTL:** ×›×©×™×© ××™×œ×™× ×‘×× ×’×œ×™×ª, ×”×ª×—×œ ××ª ×”××©×¤×˜ ×‘×¢×‘×¨×™×ª

---

**×‘×¦×¢ × ×™×ª×•×— ××§×™×£ ×‘×¤×•×¨××˜ ×”×‘×:**

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ­ **×¡× ×˜×™×× ×˜ ×•××•×•×™×¨×”**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[×—×™×•×‘×™/×©×œ×™×œ×™/××¢×•×¨×‘/××ª×•×— - ×”×¡×‘×¨ ×§×¦×¨ ××‘×•×¡×¡ ×¢×œ ××” ×©× ×××¨]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“‹ **×ª×§×¦×™×¨ ×× ×”×œ×™×**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[3-5 ××©×¤×˜×™× ×©××¡×›××™× ××ª ×¢×™×§×¨×™ ×”×©×™×—×”]
**× ×•×©××™× ×©× ×“×•× ×•:**
â€¢ × ×•×©× 1: [××™ ×”×¢×œ×”, ××” ×”×•×—×œ×˜]
â€¢ × ×•×©× 2: [××™ ×”×¢×œ×”, ××” ×”×•×—×œ×˜]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ‘¥ **××™ ×××¨ ××” (Attribution)**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[×œ×›×œ ××©×ª×ª×£ ×‘×©×™×—×”:]
**[×©×]:**
â€¢ ×¢××“×”/×”×¦×¢×” ×¢×™×§×¨×™×ª: [×¦×™×˜×•×˜ ××• ×¡×™×›×•×]
â€¢ ×ª×’×•×‘×•×ª ××¤×ª×—: [×¦×™×˜×•×˜ ××• ×¡×™×›×•×]
â€¢ ×¡×’× ×•×Ÿ ×ª×§×©×•×¨×ª: [×ª×™××•×¨ ×§×¦×¨]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” **×¤×™× ×ª ×”××•××—×”** ({', '.join(p['short_name'] for p in personas)})
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[2-3 ×ª×•×‘× ×•×ª ×¢××•×§×•×ª ×× ×§×•×“×ª ×”××‘×˜ ×©×œ ×”×¤×¨×¡×•× ×”/×•×ª ×©×œ×š]
â€¢ **×ª×•×‘× ×” 1:** [××” ×§×•×¨×” ××ª×—×ª ×œ×¤× ×™ ×”×©×˜×—?]
â€¢ **×ª×•×‘× ×” 2:** [××” ×”×“×™× ××™×§×” ×”×××™×ª×™×ª?]
â€¢ **×ª×•×‘× ×” 3 (×× ×¨×œ×•×•× ×˜×™):** [×”×–×“×× ×•×ª ×©××¤×¡×¤×¡×™×?]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… **××§×©×Ÿ ××™×™×˜××¡**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[××©×™××•×ª ×¡×¤×¦×™×¤×™×•×ª ×¢× ××—×¨××™×:]
â€¢ **[×©×]**: [××©×™××” ×§×•× ×§×¨×˜×™×ª]
â€¢ **[×©×]**: [××©×™××” ×§×•× ×§×¨×˜×™×ª]
[×× ×œ× ×–×•×”×• ××©×™××•×ª ×¡×¤×¦×™×¤×™×•×ª: "×œ× ×–×•×”×• ××©×™××•×ª ×¡×¤×¦×™×¤×™×•×ª ×‘×©×™×—×” ×–×•"]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ˆ **×¤×™×“×‘×§ ×œ×¦××™×—×” ××™×©×™×ª (Kaizen)**
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… **×œ×©×™××•×¨ (××” ×”×™×” ×˜×•×‘):**
[1-2 ×”×ª× ×”×’×•×™×•×ª, ×”×—×œ×˜×•×ª ××• ×“×¤×•×¡×™ ×ª×§×©×•×¨×ª ×—×™×•×‘×™×™× ×©×¨××•×™ ×œ×©××¨]
â€¢ [×”×ª× ×”×’×•×ª ×¡×¤×¦×™×¤×™×ª + ××™ ×¢×©×” ××•×ª×”]
[×× ××™×Ÿ ××©×”×• ×‘×•×œ×˜ ×‘××™×•×—×“: "××™×Ÿ × ×§×•×“×•×ª ×‘×•×œ×˜×•×ª ×œ×©×™××•×¨ ×‘×©×™×—×” ×–×•"]

ğŸ¯ **×œ×©×™×¤×•×¨ (×”×–×“×× ×•×ª ×œ×¦××™×—×”):**
[**×—×•×‘×”!** ×’× ×‘×©×™×—×” ××¦×•×™× ×ª ×™×© ×ª××™×“ ×”×–×“×× ×•×ª ×œ-Level Up]
â€¢ [×ª×—×•× ×¡×¤×¦×™×¤×™ ×œ×©×™×¤×•×¨ + ×“×•×’××” ××”×©×™×—×”]
â€¢ [×”×¦×¢×” ×¤×¨×§×˜×™×ª ×œ×™×™×©×•×]

ğŸ’­ **×©××œ×” ×œ××—×©×‘×”:**
[×©××œ×” ×¤×¨×•×‘×•×§×˜×™×‘×™×ª ××—×ª ×©×ª×¢×–×•×¨ ×œ×¦××™×—×” ××™×©×™×ª ××• ×œ×©×™×¤×•×¨ ×”×“×™× ××™×§×”]
"""
        return prompt
    
    async def analyze_transcript(
        self,
        segments: List[Dict],
        voice_map: Optional[Dict] = None,
        force_persona: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Perform full expert analysis on a transcript with Kaizen feedback.
        
        Flow:
        1. Build transcript text with resolved speaker names
        2. Detect context (category) using Gemini
        3. Assign 1-2 expert personas
        4. Run comprehensive analysis
        
        Args:
            segments: List of transcript segments
            voice_map: Optional mapping of speaker IDs to names
            force_persona: Optional - force a specific persona
            
        Returns:
            Dict with analysis results including Kaizen feedback
        """
        if not self.is_configured:
            return {
                "success": False,
                "error": "×©×™×¨×•×ª × ×™×ª×•×— ×”××•××—×™× ×œ× ××•×’×“×¨"
            }
        
        if not segments:
            return {
                "success": False,
                "error": "××™×Ÿ ×§×˜×¢×™ ×ª××œ×•×œ ×œ× ×™×ª×•×—"
            }
        
        voice_map = voice_map or {}
        
        # Step 1: Build transcript text
        transcript_text, speakers = self._build_transcript_text(segments, voice_map)
        print(f"ğŸ“ [Expert Analysis] Transcript: {len(transcript_text)} chars, {len(speakers)} speakers")
        
        # Step 2: Detect context
        print("ğŸ” [Expert Analysis] Step 1/3: Detecting context...")
        context = await self.detect_context(transcript_text)
        
        # Step 3: Assign personas
        if force_persona and force_persona in EXPERT_PERSONAS:
            persona_keys = [force_persona]
            print(f"ğŸ­ [Expert Analysis] Forced persona: {force_persona}")
        else:
            persona_keys = self._get_personas_for_context(context)
            print(f"ğŸ­ [Expert Analysis] Step 2/3: Assigned personas: {persona_keys}")
        
        personas = [EXPERT_PERSONAS[pk] for pk in persona_keys]
        persona_names = [p["name"] for p in personas]
        
        # Step 4: Build and run analysis
        print("ğŸ§  [Expert Analysis] Step 3/3: Running deep analysis...")
        prompt = self.build_expert_prompt(persona_keys, transcript_text, speakers, context)
        
        try:
            response = self.model.generate_content(
                prompt,
                generation_config={
                    'temperature': 0.4,
                    'max_output_tokens': 3000
                }
            )
            
            analysis_text = response.text if response.text else ""
            israel_time = get_israel_time()
            
            return {
                "success": True,
                "persona": " + ".join(persona_names),
                "persona_keys": persona_keys,
                "context": context,
                "speakers": speakers,
                "raw_analysis": analysis_text,
                "timestamp": israel_time.isoformat(),
                "timestamp_display": israel_time.strftime('%d/%m/%Y %H:%M')
            }
            
        except Exception as e:
            logger.error(f"âŒ × ×™×ª×•×— ××•××—×” × ×›×©×œ: {e}")
            return {
                "success": False,
                "error": str(e),
                "persona": " + ".join(persona_names)
            }
    
    def format_for_whatsapp(self, analysis_result: Dict, include_header: bool = True) -> str:
        """
        Format the expert analysis for WhatsApp message.
        RTL-friendly formatting with clear sections.
        
        Args:
            analysis_result: Result from analyze_transcript
            include_header: Whether to include the decorative header
            
        Returns:
            Formatted WhatsApp message string
        """
        if not analysis_result.get("success"):
            error = analysis_result.get('error', '×©×’×™××” ×œ× ×™×“×•×¢×”')
            return f"âš ï¸ ×œ× ×”×¦×œ×—×ª×™ ×œ×‘×¦×¢ × ×™×ª×•×— ××•××—×”: {error}"
        
        persona = analysis_result.get("persona", "×¢×•×–×¨ ××™×©×™")
        context = analysis_result.get("context", {})
        speakers = analysis_result.get("speakers", [])
        raw = analysis_result.get("raw_analysis", "")
        timestamp = analysis_result.get("timestamp_display", "")
        
        # Build message with RTL-friendly header
        message = ""
        
        if include_header:
            message += f"ğŸ§  *× ×™×ª×•×— ××•×¢×¦×ª ×”××•××—×™×*\n"
            message += f"ğŸ“Š ×¤×¨×¡×•× ×”: *{persona}*\n"
            message += f"ğŸ·ï¸ ×§×˜×’×•×¨×™×”: {context.get('primary_category', '×›×œ×œ×™')}"
            if context.get('secondary_category'):
                message += f" + {context.get('secondary_category')}"
            message += "\n"
            if timestamp:
                message += f"â° ×–××Ÿ: {timestamp} (×©×¢×•×Ÿ ×™×©×¨××œ)\n"
            message += "â•" * 25 + "\n\n"
        
        # Add the raw analysis (already formatted by Gemini)
        message += raw
        
        # Trim if too long for WhatsApp (4096 char limit, leave buffer)
        if len(message) > 3800:
            # Try to find a good breaking point
            # Look for the last complete section before the limit
            sections = message.split("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            truncated = ""
            for section in sections:
                if len(truncated) + len(section) + 50 < 3700:
                    truncated += section + "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
                else:
                    break
            message = truncated + "\n\n... (×”× ×™×ª×•×— ×”××œ× × ×©××¨ ×‘×“×¨×™×™×‘)"
        
        return message
    
    def save_analysis_to_drive(
        self,
        analysis_result: Dict,
        transcript_file_id: str,
        drive_service
    ) -> Optional[str]:
        """
        Save the expert analysis as a companion file to the transcript.
        This enables retroactive updates when speakers are identified.
        
        Returns:
            File ID of saved analysis, or None if failed
        """
        if not analysis_result.get("success"):
            return None
        
        try:
            # Build analysis document
            analysis_doc = {
                "persona": analysis_result.get("persona"),
                "persona_keys": analysis_result.get("persona_keys"),
                "context": analysis_result.get("context"),
                "speakers": analysis_result.get("speakers"),
                "analysis": analysis_result.get("raw_analysis"),
                "timestamp": analysis_result.get("timestamp"),
                "transcript_file_id": transcript_file_id
            }
            
            # Save via drive service if available
            # This would be implemented in drive_memory_service
            # For now, the analysis is included in the main transcript save
            return None
            
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘×©××™×¨×ª × ×™×ª×•×—: {e}")
            return None


# Singleton instance
expert_analysis_service = ExpertAnalysisService()
