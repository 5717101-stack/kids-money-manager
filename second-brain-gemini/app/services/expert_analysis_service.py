"""
Expert Analysis Service - Council of Experts for Meeting Analysis

This service applies expert personas to analyze transcribed conversations:
- Michal Dalyot / Adler Institute (Parenting/Boundaries)
- Esther Perel (Relationships/Communication)
- McKinsey & Co. / Israeli Tech (Business/Strategy)
- Simon Sinek (Leadership/The Why)

Flow:
1. Context Detection - Gemini categorizes the conversation
2. Expert Assignment - 1-2 most relevant personas assigned
3. Deep Analysis - Who said what, sentiment, insights
4. Mandatory Kaizen - Preserve/Improve feedback

Each persona provides:
- Sentiment analysis
- Executive summary (deep attribution)
- Expert insights from assigned persona(s)
- Action items (assigned to specific speakers)
- Mandatory Kaizen Feedback (×œ×©×™××•×¨ / ×œ×©×™×¤×•×¨)
"""

import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta

import google.generativeai as genai

from app.core.config import settings

logger = logging.getLogger(__name__)

# Israel timezone offset (UTC+2 winter, UTC+3 summer)
ISRAEL_TZ_OFFSET = timedelta(hours=2)


def get_israel_time() -> datetime:
    """Get current time in Israel timezone."""
    utc_now = datetime.now(timezone.utc)
    return utc_now + ISRAEL_TZ_OFFSET


# Council of Experts - Persona Definitions
EXPERT_PERSONAS = {
    "parenting": {
        "name": "××™×›×œ ×“×œ×™×•×ª / ××›×•×Ÿ ××“×œ×¨ (×”×•×¨×•×ª ×•×’×‘×•×œ×•×ª)",
        "short_name": "××›×•×Ÿ ××“×œ×¨",
        "category": "Parenting",
        "trigger_keywords": [
            "×™×œ×“×™×", "×‘× ×™×", "×‘× ×•×ª", "×‘×™×ª", "×—×™× ×•×š", "××˜×œ×•×ª", "×©×™×¢×•×¨×™ ×‘×™×ª",
            "kids", "children", "×”×•×¨×™×", "××©×¤×—×”", "×’×‘×•×œ×•×ª", "×›×œ×œ×™×",
            "×™×œ×“", "×™×œ×“×”", "×‘×Ÿ", "×‘×ª", "×”×ª× ×”×’×•×ª", "××©××¢×ª", "×”×•×¨×•×ª",
            "×’×Ÿ", "×‘×™×ª ×¡×¤×¨", "××•×¨×”", "×˜×™×¤×•×œ", "×—×•××”", "×œ×™×œ×”", "×©×™× ×”",
            "××•×›×œ", "×¡×œ×•×œ×¨", "××¡×›×™×", "×™×—×“", "××—×™×", "××—×™×•×ª"
        ],
        "focus": """
×’×™×©×ª ××“×œ×¨ ×œ×”×•×¨×•×ª ×“××•×§×¨×˜×™×ª:
- ×¢×™×“×•×“ ×‘××§×•× ×©×‘×— (Encouragement vs Praise)
- ×ª×•×¦××•×ª ×˜×‘×¢×™×•×ª ×•×”×’×™×•× ×™×•×ª (Natural & Logical Consequences)
- ×©×™×ª×•×£ ×¤×¢×•×œ×” ×•××—×¨×™×•×ª ××©×•×ª×¤×ª
- ×”×™×× ×¢×•×ª ××××‘×§×™ ×›×•×—
- ×›×‘×•×“ ×”×“×“×™ ×•×’×‘×•×œ×•×ª ×‘×¨×•×¨×™×
- ××ª×Ÿ ×‘×—×™×¨×•×ª ×‘××¡×’×¨×ª ×’×‘×•×œ×•×ª
- ×–×™×”×•×™ "××˜×¨×•×ª ××•×˜×¢×•×ª" ×‘×”×ª× ×”×’×•×ª
""",
        "tone": "×ª×•××š, ×¤×¨×§×˜×™, ×—×™× ×•×›×™, ×œ×œ× ×©×™×¤×•×˜×™×•×ª",
        "key_questions": [
            "×”×× ×”×©×™×—×” ××©×ª××©×ª ×‘×©×¤×” ××¢×•×“×“×ª ××• ×‘×‘×™×§×•×¨×ª?",
            "×”×× ×™×© ×××‘×§×™ ×›×•×— × ×¡×ª×¨×™×?",
            "×”×× ×”×’×‘×•×œ×•×ª ×©×”×•×’×“×¨×• ×‘×¨×•×¨×™× ×•×˜×‘×¢×™×™×?"
        ]
    },
    "relationship": {
        "name": "××¡×ª×¨ ×¤×¨×œ (×™×—×¡×™× ×•×ª×§×©×•×¨×ª)",
        "short_name": "××¡×ª×¨ ×¤×¨×œ",
        "category": "Relationship",
        "trigger_keywords": [
            "×‘×¢×œ", "××™×©×”", "×–×•×’×™×•×ª", "×¨×’×©×•×ª", "××”×‘×”", "relationship", "partner",
            "×¢×¨×Ÿ", "×–×•×’", "×›×¢×¡", "×ª×¡×›×•×œ", "××™× ×˜×™××™×•×ª", "×§×©×¨", "×—×™×‘×•×¨",
            "×¨×™×‘", "×•×™×›×•×—", "×œ× ××‘×™×Ÿ", "×œ× ××‘×™× ×”", "××¨×—×§", "×§×¨×‘×”",
            "×—×•×¤×©×”", "×–×•×’×™", "×‘×™×—×“", "× ×™×©×•××™×Ÿ", "×©×•×ª×¤×•×ª", "×ª××™×›×”"
        ],
        "focus": """
×’×™×©×ª ××¡×ª×¨ ×¤×¨×œ ×œ×–×•×’×™×•×ª:
- ××™× ×˜×œ×™×’× ×¦×™×” ×¨×’×©×™×ª ×¢××•×§×”
- ××™×–×•×Ÿ ×‘×™×Ÿ ×‘×™×˜×—×•×Ÿ (Security) ×œ×—×•×¤×© (Freedom)
- ×”×§×©×‘×” ×œ"×œ× × ×××¨" - ××” ×‘×™×Ÿ ×”×©×•×¨×•×ª
- ×”×‘× ×ª ×“×™× ××™×§×•×ª ×›×•×— × ×¡×ª×¨×•×ª
- ×¤×™×©×•×¨ ×‘×™×Ÿ ×ª×©×•×§×” ×œ×—×™×™ ×™×•×-×™×•×
- ×–×™×”×•×™ ×“×¤×•×¡×™× ×—×•×–×¨×™× ×‘×ª×§×©×•×¨×ª
- ×”×¢×¨×›×ª "×¡×’× ×•× ×•×ª ×”×ª×§×©×¨×•×ª" (Attachment Styles)
""",
        "tone": "×××¤×ª×™, ×¢××•×§, ×ª×•×‘× ×ª×™, ×œ×œ× ×©×™×¤×•×˜",
        "key_questions": [
            "××” ×œ× × ×××¨ ×‘×©×™×—×”?",
            "××™×–×” ×¨×’×© × ×¡×ª×¨ ××ª×—×ª ×œ×¤× ×™ ×”×©×˜×—?",
            "×”×× ×™×© ×“×¤×•×¡ ×—×•×–×¨ ×©×¨××•×™ ×œ×©×™× ×œ×‘ ××œ×™×•?"
        ]
    },
    "strategy": {
        "name": "××§×™× ×–×™ ×•×˜×§ ×™×©×¨××œ×™ (××¡×˜×¨×˜×’×™×” ×•×¢×¡×§×™×)",
        "short_name": "××§×™× ×–×™ + Tech",
        "category": "Business",
        "trigger_keywords": [
            "×¢×¡×§", "××•×¦×¨", "×œ×§×•×—×•×ª", "××¡×˜×¨×˜×’×™×”", "roadmap", "MVP", "startup",
            "product", "×›×¡×£", "×ª×§×¦×™×‘", "×”×›× ×¡×•×ª", "××›×™×¨×•×ª", "×©×•×§", "××ª×—×¨×™×",
            "×¤×™×¦'×¨", "×¤×™×ª×•×—", "tech", "×§×•×“", "××©×§×™×¢×™×", "×’×™×•×¡", "×—×‘×¨×”",
            "×¢×¡×§×™×", "×¤×¨×•×™×§×˜", "×“×“×œ×™×™×Ÿ", "×™×¢×“×™×", "KPI", "OKR", "×‘×™×¦×•×¢×™×",
            "×ª×›× ×•×Ÿ", "××¤×œ×™×§×¦×™×”", "××ª×¨", "×©×™×¨×•×ª", "B2B", "B2C", "SaaS"
        ],
        "focus": """
×’×™×©×ª ××§×™× ×–×™ + High-Tech ×™×©×¨××œ×™:
- ××‘× ×” MECE (Mutually Exclusive, Collectively Exhaustive)
- × ×™×ª×•×— ××‘×•×¡×¡ × ×ª×•× ×™× (Data-Driven)
- ×¡×§×™×™×œ×‘×™×œ×™×•×ª ×•×™×¢×™×œ×•×ª
- ×—×©×™×‘×ª Agile/Lean Startup
- MVP ×•××™×˜×¨×¦×™×•×ª ××”×™×¨×•×ª
- ×”×’×“×¨×ª KPIs ×‘×¨×•×¨×™×
- ×ª×¢×“×•×£ ××›×–×¨×™ (Ruthless Prioritization)
- "First Principles Thinking"
""",
        "tone": "×—×“, ××§×¦×•×¢×™, ×××•×§×“ ×¤×¢×•×œ×”, ×—×•×ª×š ×œ×¢× ×™×™×Ÿ",
        "key_questions": [
            "××” ×”-ROI ×”×¦×¤×•×™?",
            "××” ×”×¦×¢×“ ×”×‘× ×”×§×˜×Ÿ ×‘×™×•×ª×¨ ×©××‘×™× ×¢×¨×š?",
            "××” ×”-bottleneck ×”×××™×ª×™?"
        ]
    },
    "leadership": {
        "name": "×¡×™×™××•×Ÿ ×¡×™× ×§ (×× ×”×™×’×•×ª ×•×”×©×¨××”)",
        "short_name": "×¡×™×™××•×Ÿ ×¡×™× ×§",
        "category": "Leadership",
        "trigger_keywords": [
            "×¦×•×•×ª", "×¢×•×‘×“×™×", "×× ×”×œ", "×—×‘×¨×”", "×ª×¨×‘×•×ª", "hiring", "mentoring",
            "culture", "×× ×”×™×’×•×ª", "×”×©×¨××”", "×¢×¨×›×™×", "×—×–×•×Ÿ", "××©××¢×•×ª",
            "××•×˜×™×‘×¦×™×”", "×’×™×•×¡", "×¤×™×˜×•×¨×™×", "×‘×™×¦×•×¢×™×", "××©×•×‘", "feedback",
            "1on1", "××—×“ ×¢×œ ××—×“", "×¦××™×—×”", "×§×¨×™×™×¨×”", "×¤×•×˜× ×¦×™××œ"
        ],
        "focus": """
×’×™×©×ª ×¡×™×™××•×Ÿ ×¡×™× ×§ ×œ×× ×”×™×’×•×ª:
- Start with Why - ×ª×ª×—×™×œ ××”"×œ××”"
- The Infinite Game - ××©×—×§ ××™× ×¡×•×¤×™, ×œ× × ×™×¦×—×•×Ÿ ×—×“ ×¤×¢××™
- Circle of Safety - ×™×¦×™×¨×ª ××¢×’×œ ×‘×™×˜×—×•×Ÿ ×œ×¦×•×•×ª
- Leaders Eat Last - ×× ×”×™×’ ×“×•××’ ×§×•×“× ×œ××—×¨×™×
- ×× ×”×™×’×•×ª ××©×¨×ª×ª (Servant Leadership)
- ×‘× ×™×™×ª ×××•×Ÿ ×œ×˜×•×•×— ××¨×•×š
- "Optimism fueled by reality"
""",
        "tone": "××¢×•×¨×¨ ×”×©×¨××”, ×××•×§×“ ×‘×× ×©×™×, ×—×–×•× ×™, ×× ×•×©×™",
        "key_questions": [
            "××” ×”'×œ××”' ×××—×•×¨×™ ×”×”×—×œ×˜×”?",
            "×”×× ×–×” ×‘×•× ×” ×××•×Ÿ ×œ×˜×•×•×— ××¨×•×š?",
            "×”×× ×”×× ×”×™×’×•×ª ×›××Ÿ ××©×¨×ª×ª ××• ×©×•×œ×˜×ª?"
        ]
    },
    "general": {
        "name": "×¢×•×–×¨ ××™×©×™ ×—×›×",
        "short_name": "×¢×•×–×¨ ××™×©×™",
        "category": "General",
        "trigger_keywords": [],
        "focus": "×¡×™×›×•× ×‘×¨×•×¨ ×•×ª××¦×™×ª×™ ×¢× ×ª×•×‘× ×•×ª ×¤×¨×§×˜×™×•×ª ×•××§×©×Ÿ ××™×™×˜××¡",
        "tone": "×™×©×™×¨, ×©×™××•×©×™, ×¤×¨×§×˜×™",
        "key_questions": ["××” ×”× ×§×•×“×•×ª ×”×¢×™×§×¨×™×•×ª?", "××” ×”×¦×¢×“ ×”×‘×?"]
    }
}


class ExpertAnalysisService:
    """
    Council of Experts Analysis System for meeting transcripts.
    
    Flow:
    1. Context Detection - Ask Gemini to categorize the transcript
    2. Expert Assignment - Select 1-2 relevant personas
    3. Deep Analysis - Full analysis with attribution
    4. Mandatory Kaizen - Preserve/Improve feedback
    """
    
    def __init__(self):
        self.api_key = settings.google_api_key
        self.model = None
        self.model_name = None
        
        if self.api_key:
            genai.configure(api_key=self.api_key)
            
            # Try models in order of preference (same as gemini_service.py)
            # gemini-2.5-pro is the stable model that works
            models_to_try = [
                'gemini-2.5-pro',          # Primary - stable and works
                'gemini-2.0-flash',        # Fallback 1 - newer flash
                'gemini-1.5-flash-latest', # Fallback 2 - flash with latest suffix
                'gemini-pro',              # Fallback 3 - basic pro
            ]
            
            for model_name in models_to_try:
                try:
                    self.model = genai.GenerativeModel(model_name)
                    self.model_name = model_name
                    logger.info(f"âœ… ×©×™×¨×•×ª × ×™×ª×•×— ×”××•××—×™× ××•×ª×—×œ ×¢× {model_name}")
                    print(f"âœ… [Expert Analysis] Model initialized: {model_name}")
                    break
                except Exception as e:
                    logger.warning(f"âš ï¸  Could not init {model_name}: {e}")
                    print(f"âš ï¸  [Expert Analysis] Model {model_name} failed: {e}")
                    continue
            
            if not self.model:
                logger.error("âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ××ª×—×œ ××£ ××•×“×œ")
                print("âŒ [Expert Analysis] All models failed to initialize!")
        else:
            logger.warning("âš ï¸  ××¤×ª×— Google API ×œ× ××•×’×“×¨ - × ×™×ª×•×— ××•××—×™× ××•×©×‘×ª")
        
        self.is_configured = bool(self.api_key and self.model)
    
    def _build_transcript_text(self, segments: List[Dict], voice_map: Dict) -> Tuple[str, List[str]]:
        """
        Build readable transcript with speaker names resolved.
        Returns (transcript_text, list_of_speakers).
        """
        transcript_lines = []
        speakers_set = set()
        
        for seg in segments:
            speaker_id = seg.get("speaker", "×“×•×‘×¨ ×œ× ××–×•×”×”")
            # Replace speaker ID with name if known
            speaker_name = voice_map.get(speaker_id.lower(), speaker_id)
            if speaker_name == speaker_id:
                # Try without case sensitivity
                for key, value in voice_map.items():
                    if key.lower() == speaker_id.lower():
                        speaker_name = value
                        break
            
            speakers_set.add(speaker_name)
            text = seg.get("text", "")
            transcript_lines.append(f"**{speaker_name}**: {text}")
        
        return "\n".join(transcript_lines), list(speakers_set)
    
    async def detect_context(self, transcript_text: str) -> Dict[str, Any]:
        """
        Step 1: Use Gemini to categorize the conversation.
        
        Returns:
            {
                "primary_category": "Business|Parenting|Relationship|Leadership|General",
                "secondary_category": "..." (optional),
                "confidence": 0.0-1.0,
                "reasoning": "..."
            }
        """
        if not self.is_configured:
            return {"primary_category": "General", "confidence": 0.5, "reasoning": "Model not configured"}
        
        detection_prompt = f"""××ª×” ××•××—×” ×œ× ×™×ª×•×— ×©×™×—×•×ª. ×§×˜×’×¨ ××ª ×”×©×™×—×” ×”×‘××” ×œ××—×ª ××”×§×˜×’×•×¨×™×•×ª:

1. **Parenting** - ×©×™×—×•×ª ×¢×œ ×™×œ×“×™×, ×—×™× ×•×š, ××©×¤×—×”, ×’×‘×•×œ×•×ª ×‘×‘×™×ª
2. **Relationship** - ×©×™×—×•×ª ×¢×œ ×–×•×’×™×•×ª, ×™×—×¡×™×, ×¨×’×©×•×ª ×‘×™×Ÿ ×‘× ×™ ×–×•×’
3. **Business** - ×©×™×—×•×ª ×¢×œ ×¢×¡×§×™×, ×˜×›× ×•×œ×•×’×™×”, ×¤×¨×•×™×§×˜×™×, ×›×¡×£, ×¡×˜××¨×˜××¤
4. **Leadership** - ×©×™×—×•×ª ×¢×œ × ×™×”×•×œ ×¦×•×•×ª, ×ª×¨×‘×•×ª ××¨×’×•× ×™×ª, ×× ×˜×•×¨×™× ×’
5. **General** - ×©×™×—×•×ª ×›×œ×œ×™×•×ª ×©×œ× ××ª××™××•×ª ×œ××£ ×§×˜×’×•×¨×™×” ×¡×¤×¦×™×¤×™×ª

**×ª××œ×™×œ ×”×©×™×—×”:**
{transcript_text[:3000]}

**×”×—×–×¨ ×ª×©×•×‘×” ×‘×¤×•×¨××˜ JSON ×‘×œ×‘×“:**
{{
    "primary_category": "×§×˜×’×•×¨×™×” ×¨××©×™×ª",
    "secondary_category": "×§×˜×’×•×¨×™×” ××©× ×™×ª ××• null",
    "confidence": 0.8,
    "reasoning": "×”×¡×‘×¨ ×§×¦×¨"
}}
"""
        
        try:
            print(f"ğŸ” [Context Detection] Calling Gemini model: {self.model_name}")
            response = self.model.generate_content(
                detection_prompt,
                generation_config={
                    'temperature': 0.1,
                    'max_output_tokens': 500
                }
            )
            
            # Parse JSON response
            response_text = response.text.strip()
            print(f"ğŸ” [Context Detection] Response received: {len(response_text)} chars")
            # Extract JSON from possible markdown code block
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(response_text)
            print(f"ğŸ¯ [Context Detection] Primary: {result.get('primary_category')}, Secondary: {result.get('secondary_category')}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Context detection failed: {e}")
            # Fallback to keyword-based detection
            return self._fallback_context_detection(transcript_text)
    
    def _fallback_context_detection(self, transcript_text: str) -> Dict[str, Any]:
        """Fallback to keyword-based detection if Gemini fails."""
        text_lower = transcript_text.lower()
        
        scores = {}
        for persona_key, persona in EXPERT_PERSONAS.items():
            if persona_key == "general":
                continue
            score = sum(1 for kw in persona["trigger_keywords"] if kw in text_lower)
            scores[persona_key] = score
        
        if not scores or max(scores.values()) == 0:
            return {"primary_category": "General", "confidence": 0.3, "reasoning": "No keywords matched"}
        
        # Map persona keys to categories
        category_map = {
            "parenting": "Parenting",
            "relationship": "Relationship",
            "strategy": "Business",
            "leadership": "Leadership"
        }
        
        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        primary = sorted_scores[0]
        secondary = sorted_scores[1] if len(sorted_scores) > 1 and sorted_scores[1][1] > 0 else None
        
        return {
            "primary_category": category_map.get(primary[0], "General"),
            "secondary_category": category_map.get(secondary[0], None) if secondary else None,
            "confidence": min(1.0, primary[1] / 10),
            "reasoning": f"Keyword match: {primary[1]} for {primary[0]}"
        }
    
    def _get_personas_for_context(self, context: Dict[str, Any]) -> List[str]:
        """
        Map context categories to EXACTLY ONE persona key.
        STRICT ROUTING: Never mix unrelated experts.
        """
        category_to_persona = {
            "Parenting": "parenting",      # -> Michal Dalyot / Adler
            "Relationship": "relationship", # -> Esther Perel
            "Business": "strategy",         # -> McKinsey + Tech
            "Leadership": "strategy",       # -> McKinsey (leadership is business)
            "General": "general"            # -> Generic assistant
        }
        
        primary = context.get("primary_category", "General")
        persona = category_to_persona.get(primary, "general")
        
        print(f"ğŸ¯ [Persona Routing] Category '{primary}' -> Persona '{persona}'")
        
        # STRICT: Return only ONE persona, never mix
        return [persona]
    
    def build_expert_prompt(
        self, 
        persona_keys: List[str], 
        transcript_text: str, 
        speakers: List[str],
        context: Dict[str, Any]
    ) -> str:
        """
        Build DEEP analysis prompt with STRICT format.
        
        Structure (100 words total):
        - Executive Summary: 30 words max
        - Expert Insight: 50 words (ONE deep insight)
        - Action Items: Mandatory if exist
        - Kaizen: 1 preserve, 1 improve
        """
        # Get the ONE persona
        persona = EXPERT_PERSONAS.get(persona_keys[0], EXPERT_PERSONAS["general"])
        speakers_str = ", ".join(speakers) if speakers else "×“×•×‘×¨×™× ×œ× ××–×•×”×™×"
        category = context.get('primary_category', '×›×œ×œ×™')
        
        # Keep more transcript for context (4000 chars)
        if len(transcript_text) > 4000:
            transcript_text = transcript_text[:4000] + "\n...(×§×•×¦×¨)"
        
        # Build persona-specific insight question
        insight_focus = {
            "parenting": "××” ×”×“×™× ××™×§×” ×”×”×•×¨×™×ª ×”×××™×ª×™×ª? ×”×× ×™×© ×××‘×§×™ ×›×•×— × ×¡×ª×¨×™×? ×”×× ×”×’×‘×•×œ×•×ª ×‘×¨×•×¨×™×?",
            "relationship": "××” ×œ× × ×××¨? ××™×–×” ×¨×’×© × ×¡×ª×¨ ××ª×—×ª ×œ×¤× ×™ ×”×©×˜×—? ××” ×”×“×¤×•×¡ ×”×—×•×–×¨?",
            "strategy": "××” ×”-ROI ×”×××™×ª×™? ××™×¤×” ×”×¡×™×›×•×Ÿ ×”×’×“×•×œ? ××” ×”-action ×”×›×™ ×§×¨×™×˜×™?",
            "leadership": "××” ×”-'Why' ×”×××™×ª×™? ×”×× ×™×© 'Circle of Safety'? ××” ×™×¢×•×¨×¨ ×”×©×¨××”?",
            "general": "××” ×¢×™×§×¨ ×”×©×™×—×” ×•××” ×¦×¨×™×š ×œ×§×¨×•×ª ×”×œ××”?"
        }.get(persona_keys[0], "××” ×”×ª×•×‘× ×” ×”×¢×™×§×¨×™×ª?")
        
        prompt = f"""××ª×” {persona['name']} - ××•××—×” ×‘×ª×—×•××š.

**×§×˜×’×•×¨×™×”:** {category}
**××©×ª×ª×¤×™×:** {speakers_str}

**×ª××œ×™×œ ×”×©×™×—×”:**
{transcript_text}

---

**×”××©×™××” ×©×œ×š:** ×¡×¤×§ × ×™×ª×•×— ×¢××•×§ ×•×××•×§×“. ×”×™×” ×¡×¤×¦×™×¤×™ - ×¦×™×™×Ÿ ××™ ×××¨ ××”.

**×©××œ×ª ×”××¤×ª×— ×©×œ×š:** {insight_focus}

---

**×¤×•×¨××˜ ×”×ª×©×•×‘×” (×¢×‘×¨×™×ª, RTL):**

ğŸ¯ ×¡× ×˜×™×× ×˜: [×—×™×•×‘×™/××ª×•×—/××¢×•×¨×‘] - [××©×¤×˜ ××—×“]

ğŸ“‹ ×ª××¦×™×ª (30 ××™×œ×™×):
â€¢ [×©×] ×”×¢×œ×”/×”×¦×™×¢: [××”]
â€¢ [×©×] ×”×’×™×‘/×”×¡×›×™×: [××”]
â€¢ ×”×”×—×œ×˜×”/×”××¡×§× ×”: [××”]

ğŸ” ×ª×•×‘× ×ª {persona['short_name']}:
[2-3 ××©×¤×˜×™× ×¢× ×ª×•×‘× ×” ×¢××•×§×” ×× ×§×•×“×ª ×”××‘×˜ ×©×œ {persona['short_name']}. 
×”×ª×™×™×—×¡ ×œ: {insight_focus}
×”×™×” ×¡×¤×¦×™×¤×™ - ×”×¤× ×” ×œ×“×•×’×××•×ª ××”×©×™×—×”.]

âœ… ××©×™××•×ª:
â€¢ *[×©×]*: [××©×™××” ×§×•× ×§×¨×˜×™×ª]
â€¢ *[×©×]*: [××©×™××” ×§×•× ×§×¨×˜×™×ª]
(×× ××™×Ÿ ××©×™××•×ª ×‘×¨×•×¨×•×ª: "×œ× ×–×•×”×• ××©×™××•×ª ×¡×¤×¦×™×¤×™×•×ª")

ğŸ“ˆ ×§××™×–×Ÿ:
âœ“ ×œ×©×™××•×¨: [×”×ª× ×”×’×•×ª/×”×—×œ×˜×” ×—×™×•×‘×™×ª ×¡×¤×¦×™×¤×™×ª ×©×¨××™× ×• ×‘×©×™×—×”]
â†’ ×œ×©×™×¤×•×¨: [×”×–×“×× ×•×ª ×§×•× ×§×¨×˜×™×ª ××—×ª ×œ×¦××™×—×” + ×”××œ×¦×” ×¤×¨×§×˜×™×ª]
"""
        return prompt
    
    async def analyze_transcript(
        self,
        segments: List[Dict],
        voice_map: Optional[Dict] = None,
        force_persona: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Perform full expert analysis on a transcript with Kaizen feedback.
        
        Flow:
        1. Build transcript text with resolved speaker names
        2. Detect context (category) using Gemini
        3. Assign 1-2 expert personas
        4. Run comprehensive analysis
        
        Args:
            segments: List of transcript segments
            voice_map: Optional mapping of speaker IDs to names
            force_persona: Optional - force a specific persona
            
        Returns:
            Dict with analysis results including Kaizen feedback
        """
        if not self.is_configured:
            return {
                "success": False,
                "error": "×©×™×¨×•×ª × ×™×ª×•×— ×”××•××—×™× ×œ× ××•×’×“×¨"
            }
        
        if not segments:
            return {
                "success": False,
                "error": "××™×Ÿ ×§×˜×¢×™ ×ª××œ×•×œ ×œ× ×™×ª×•×—"
            }
        
        voice_map = voice_map or {}
        
        # Step 1: Build transcript text
        transcript_text, speakers = self._build_transcript_text(segments, voice_map)
        print(f"ğŸ“ [Expert Analysis] Transcript: {len(transcript_text)} chars, {len(speakers)} speakers")
        
        # Step 2: Detect context
        print("ğŸ” [Expert Analysis] Step 1/3: Detecting context...")
        context = await self.detect_context(transcript_text)
        
        # Step 3: Assign personas
        if force_persona and force_persona in EXPERT_PERSONAS:
            persona_keys = [force_persona]
            print(f"ğŸ­ [Expert Analysis] Forced persona: {force_persona}")
        else:
            persona_keys = self._get_personas_for_context(context)
            print(f"ğŸ­ [Expert Analysis] Step 2/3: Assigned personas: {persona_keys}")
        
        personas = [EXPERT_PERSONAS[pk] for pk in persona_keys]
        persona_names = [p["name"] for p in personas]
        
        # Step 4: Build and run analysis with RETRY logic
        print(f"ğŸ§  [Expert Analysis] Step 3/3: Running deep analysis with model: {self.model_name}")
        prompt = self.build_expert_prompt(persona_keys, transcript_text, speakers, context)
        print(f"ğŸ“ [Expert Analysis] Prompt length: {len(prompt)} chars")
        
        analysis_text = ""
        max_retries = 2
        
        for attempt in range(max_retries):
            try:
                print(f"   ğŸ”„ Attempt {attempt + 1}/{max_retries}")
                
                # Use simpler prompt on retry
                current_prompt = prompt if attempt == 0 else self._build_fallback_prompt(transcript_text, speakers)
                
                response = self.model.generate_content(
                    current_prompt,
                    generation_config={
                        'temperature': 0.3 if attempt > 0 else 0.4,
                        'max_output_tokens': 1000
                    }
                )
                
                analysis_text = response.text if response.text else ""
                print(f"   ğŸ“ Response: {len(analysis_text)} chars")
                
                # Check for empty response
                if len(analysis_text.strip()) < 50:
                    print(f"   âš ï¸  Response too short ({len(analysis_text)} chars), retrying...")
                    continue
                
                # SUCCESS - break out of retry loop
                break
                
            except Exception as e:
                print(f"   âŒ Attempt {attempt + 1} failed: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"âŒ [CRITICAL] × ×™×ª×•×— ××•××—×” × ×›×©×œ ×œ××—×¨ {max_retries} × ×¡×™×•× ×•×ª: {e}")
                    import traceback
                    traceback.print_exc()
        
        # Final validation
        if not analysis_text or len(analysis_text.strip()) < 50:
            print("âŒ [CRITICAL] Analysis returned EMPTY after all retries!")
            logger.error("âŒ [CRITICAL] Expert analysis returned empty text")
            return {
                "success": False,
                "error": "× ×™×ª×•×— ×—×–×¨ ×¨×™×§ - ×‘×“×•×§ ××ª ×”××•×“×œ",
                "persona": " + ".join(persona_names),
                "model_used": self.model_name
            }
        
        print(f"âœ… [Expert Analysis] SUCCESS - {len(analysis_text)} chars")
        israel_time = get_israel_time()
        
        return {
            "success": True,
            "persona": " + ".join(persona_names),
            "persona_keys": persona_keys,
            "context": context,
            "speakers": speakers,
            "raw_analysis": analysis_text,
            "timestamp": israel_time.isoformat(),
            "timestamp_display": israel_time.strftime('%d/%m/%Y %H:%M')
        }
    
    def _build_fallback_prompt(self, transcript_text: str, speakers: List[str]) -> str:
        """Simple fallback prompt when main prompt fails."""
        speakers_str = ", ".join(speakers) if speakers else "×“×•×‘×¨×™× ×œ× ×™×“×•×¢×™×"
        
        return f"""×¡×›× ××ª ×”×©×™×—×” ×”×‘××” ×‘×¢×‘×¨×™×ª.

**××©×ª×ª×¤×™×:** {speakers_str}

**×ª××œ×™×œ:**
{transcript_text[:3000]}

**×ª×©×•×‘×” ×‘×¤×•×¨××˜ ×”×‘×:**

ğŸ¯ ×¡× ×˜×™×× ×˜: [×—×™×•×‘×™/×©×œ×™×œ×™/××¢×•×¨×‘]

ğŸ“‹ ×ª××¦×™×ª:
â€¢ [××™ ×××¨ ××” - × ×§×•×“×” 1]
â€¢ [××™ ×××¨ ××” - × ×§×•×“×” 2]

âœ… ××©×™××•×ª:
â€¢ [×©×]: [××©×™××”]

ğŸ“ˆ ×§××™×–×Ÿ:
âœ“ ×œ×©×™××•×¨: [× ×§×•×“×” ×—×™×•×‘×™×ª]
â†’ ×œ×©×™×¤×•×¨: [×”×–×“×× ×•×ª ×œ×¦××™×—×”]
"""
    
    def format_for_whatsapp(self, analysis_result: Dict, include_header: bool = True) -> str:
        """
        Format the expert analysis for WhatsApp message.
        
        STRICT: Total message must be under 1600 characters.
        
        Args:
            analysis_result: Result from analyze_transcript
            include_header: Whether to include the decorative header
            
        Returns:
            Formatted WhatsApp message string (max 1600 chars)
        """
        if not analysis_result.get("success"):
            error = analysis_result.get('error', '×©×’×™××” ×œ× ×™×“×•×¢×”')
            return f"âš ï¸ ×œ× ×”×¦×œ×—×ª×™ ×œ×‘×¦×¢ × ×™×ª×•×— ××•××—×”: {error}"
        
        persona = analysis_result.get("persona", "×¢×•×–×¨ ××™×©×™")
        context = analysis_result.get("context", {})
        raw = analysis_result.get("raw_analysis", "")
        
        # Debug logging
        print(f"ğŸ“Š [format_for_whatsapp] Raw analysis: {len(raw)} chars")
        if not raw:
            print("   âš ï¸  WARNING: raw_analysis is EMPTY!")
        
        # Build message with minimal header
        message = ""
        
        if include_header:
            # Minimal but informative header
            category = context.get('primary_category', '×›×œ×œ×™')
            message += f"ğŸ§  *{persona}*\n"
            message += f"ğŸ“‚ {category}\n"
            message += "â”€" * 18 + "\n\n"
        
        # Add the raw analysis (formatted by Gemini)
        message += raw
        
        # Ensure message fits WhatsApp (max ~4000, target ~2000)
        # If over 2000, trim gracefully
        if len(message) > 2000:
            # Find a good breaking point at section boundary
            for section_marker in ["ğŸ“ˆ ×§××™×–×Ÿ", "âœ… ××©×™××•×ª", "ğŸ” ×ª×•×‘× ×ª"]:
                cutoff = message.rfind(section_marker)
                if 1200 < cutoff < 1900:
                    message = message[:cutoff + 200]  # Include some of the section
                    # Find the next newline to break cleanly
                    next_newline = message.rfind('\n')
                    if next_newline > 1200:
                        message = message[:next_newline]
                    message += "\n\n_(×¡×™×›×•× ××§×•×¦×¨ - ×”× ×™×ª×•×— ×”××œ× ×‘×“×¨×™×™×‘)_"
                    break
            else:
                # Fallback: just truncate at newline
                cutoff = message[:1900].rfind('\n')
                if cutoff > 1200:
                    message = message[:cutoff] + "\n\n_(×§×•×¦×¨)_"
        
        print(f"ğŸ“Š [format_for_whatsapp] Final message: {len(message)} chars")
        return message
    
    def save_analysis_to_drive(
        self,
        analysis_result: Dict,
        transcript_file_id: str,
        drive_service
    ) -> Optional[str]:
        """
        Save the expert analysis as a companion file to the transcript.
        This enables retroactive updates when speakers are identified.
        
        Returns:
            File ID of saved analysis, or None if failed
        """
        if not analysis_result.get("success"):
            return None
        
        try:
            # Build analysis document
            analysis_doc = {
                "persona": analysis_result.get("persona"),
                "persona_keys": analysis_result.get("persona_keys"),
                "context": analysis_result.get("context"),
                "speakers": analysis_result.get("speakers"),
                "analysis": analysis_result.get("raw_analysis"),
                "timestamp": analysis_result.get("timestamp"),
                "transcript_file_id": transcript_file_id
            }
            
            # Save via drive service if available
            # This would be implemented in drive_memory_service
            # For now, the analysis is included in the main transcript save
            return None
            
        except Exception as e:
            logger.error(f"âŒ ×©×’×™××” ×‘×©××™×¨×ª × ×™×ª×•×—: {e}")
            return None


# Singleton instance
expert_analysis_service = ExpertAnalysisService()
